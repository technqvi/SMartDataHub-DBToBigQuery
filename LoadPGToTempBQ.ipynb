{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imported Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import psycopg2.extras as extras\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime,timezone\n",
    "from dateutil import tz\n",
    "\n",
    "import os\n",
    "import sys \n",
    "\n",
    "from configupdater import ConfigUpdater\n",
    "# pip install ConfigUpdater\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "from google.api_core.exceptions import BadRequest\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "import bq_storage_api.incident_data_pb2 as pb2_incident\n",
    "\n",
    "# [metadata]\n",
    "# pmr_pm_plan = 2019-01-01 00:00:00\n",
    "# pmr_pm_item = 2019-01-01 00:00:00\n",
    "# pmr_project = 2019-01-01 00:00:00\n",
    "# pmr_inventory  = 2019-01-01 00:00:00\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "way=\"bq-storage-api\" # 1=\"merge\"  or \"bq-storage-api\"\n",
    "view_name = \"xyz_incident\"\n",
    "\n",
    "\n",
    "# way=\"merge\" \n",
    "# view_name = \"pmr_pm_plan\"\n",
    "\n",
    "is_py=False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View name to load to BQ :xyz_incident and data name: incident\n"
     ]
    }
   ],
   "source": [
    "data_name=view_name.replace(\"pmr_\",\"\").replace(\"xyz_\",\"\")\n",
    "\n",
    "                             \n",
    "isFirstLoad=False\n",
    "                             \n",
    "\n",
    "if is_py:\n",
    "    press_Y=''\n",
    "    ok=False\n",
    "\n",
    "    if len(sys.argv) > 1:\n",
    "        view_name=sys.argv[1]\n",
    "    else:\n",
    "        print(\"Enter the following input: \")\n",
    "        view_name = input(\"View Table Name : \")\n",
    "print(f\"View name to load to BQ :{view_name} and data name: {data_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imported date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTC: 2024-01-11 10:35:56 For This Import\n"
     ]
    }
   ],
   "source": [
    "dt_imported=datetime.now(timezone.utc) # utc\n",
    "dt_imported=datetime.strptime(dt_imported.strftime(\"%Y-%m-%d %H:%M:%S\"),\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"UTC: {dt_imported} For This Import\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set view data and log table and protocolbuffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18  -  incident_id\n"
     ]
    }
   ],
   "source": [
    "log = \"models_logging_change\"\n",
    "\n",
    "data_pb2=None\n",
    "sp=f\"merge_{data_name}\"\n",
    "\n",
    "def get_process_configuration_data(view_name):\n",
    "    \n",
    "    x_data_pb2=None\n",
    "    if view_name == \"pmr_pm_plan\":\n",
    "        tableContentID = 36\n",
    "        key_name = \"pm_id\"\n",
    "        changed_field_mapping=['planned_date','ended_pm_date',\n",
    "                               'project_id','remark','team_lead_id']\n",
    "\n",
    "    elif view_name == \"pmr_pm_item\":\n",
    "        tableContentID = 37\n",
    "        key_name = \"pm_item_id\"\n",
    "\n",
    "\n",
    "    elif view_name == \"pmr_project\":\n",
    "        tableContentID = 7\n",
    "        key_name = \"project_id\"\n",
    "\n",
    "\n",
    "    elif view_name == \"pmr_inventory\":\n",
    "        tableContentID = 14\n",
    "        key_name = \"inventory_id\"\n",
    "\n",
    "        \n",
    "    elif view_name == \"xyz_incident\":\n",
    "        tableContentID = 18\n",
    "        key_name = \"incident_id\"   \n",
    "        x_data_pb2=pb2_incident.IncidentData()\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"No specified content type id\")\n",
    "        \n",
    "    return tableContentID, key_name,sp,x_data_pb2\n",
    "\n",
    "\n",
    "                             \n",
    "                               \n",
    "content_id , view_name_id,sp_name,data_pb2=get_process_configuration_data(view_name)\n",
    "print(content_id,\" - \",view_name_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check configuration parameter validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bq-storage-api\n",
      "<google.protobuf.pyext._message.MessageDescriptor object at 0x0000023344F685B0>\n"
     ]
    }
   ],
   "source": [
    "#  # 1=\"merge\"  or \"bq-storage-api\"\n",
    "def check_config_parameter_validation(way,sp,data_pb2):\n",
    "  if  (way==\"merge\") and sp is None:\n",
    "     raise Exception(f\"StoreProcedure is not allowed to None in {way} Way.\")\n",
    "  elif  (way==\"bq-storage-api\") and data_pb2 is None:\n",
    "     raise Exception(f\"ProtoBuf Data is not allowed to None in {way} Way.\")   \n",
    "  return True\n",
    "    \n",
    "result_data_validation=check_config_parameter_validation(way,sp,data_pb2)\n",
    "if result_data_validation and  way==\"merge\":\n",
    " print(f\"{way} - {sp}\")\n",
    "elif result_data_validation and  way==\"bq-storage-api\":\n",
    " print(f\"{way}\")\n",
    " print(data_pb2.DESCRIPTOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set data and cofig path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".env\n",
      "cfg_last_import\n"
     ]
    }
   ],
   "source": [
    "# Test config,env file and key to be used ,all of used key  are existing.\n",
    "cfg_path=\"cfg_last_import\"\n",
    "env_path='.env'\n",
    "\n",
    "updater = ConfigUpdater()\n",
    "updater.read(os.path.join(cfg_path,f\"{view_name}.cfg\"))\n",
    "\n",
    "config = dotenv_values(dotenv_path=env_path)\n",
    "\n",
    "print(env_path)\n",
    "print(cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pongthorn.SMartData_Temp.temp_incident\n",
      "pongthorn.SMartDataAnalytics.incident\n"
     ]
    }
   ],
   "source": [
    "# Test exsitng project dataset and table anme\n",
    "\n",
    "projectId=config['PROJECT_ID']  # smart-data-ml  or kku-intern-dataai or ponthorn\n",
    "credential_file=config['PROJECT_CREDENTIAL_FILE']\n",
    "# C:\\Windows\\smart-data-ml-91b6f6204773.json\n",
    "# C:\\Windows\\kku-intern-dataai-a5449aee8483.json\n",
    "# C:\\Windows\\pongthorn-5decdc5124f5.json\n",
    "\n",
    "\n",
    "dataset_id='SMartData_Temp'  # 'SMartData_Temp'  'PMReport_Temp'\n",
    "main_dataset_id='SMartDataAnalytics'  # ='SMartDataAnalytics'  'PMReport_Main'\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(credential_file)\n",
    "\n",
    "table_name=f\"temp_{data_name}\" #can change in (\"name\") to temp table\n",
    "table_id = f\"{projectId}.{dataset_id}.{table_name}\"\n",
    "print(table_id)\n",
    "\n",
    "\n",
    "main_table_name=data_name\n",
    "main_table_id = f\"{projectId}.{main_dataset_id}.{main_table_name}\"\n",
    "print(main_table_id)\n",
    "\n",
    "# https://cloud.google.com/bigquery/docs/reference/rest/v2/Job\n",
    "to_bq_mode=\"WRITE_EMPTY\"\n",
    "\n",
    "\n",
    "client = bigquery.Client(credentials= credentials,project=projectId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Configuration File and Initialize BQ Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incident - UTC:2024-01-01 00:00:00  Of Last Import\n"
     ]
    }
   ],
   "source": [
    "last_imported=datetime.strptime(updater[\"metadata\"][view_name].value,\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"{data_name} - UTC:{last_imported}  Of Last Import\")\n",
    "\n",
    "# local_zone = tz.tzlocal()\n",
    "# last_imported = last_imported.astimezone(local_zone)\n",
    "# print(f\"Local Asia/Bangkok:{last_imported}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postgres &BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_postgres_conn():\n",
    " try:\n",
    "  conn = psycopg2.connect(\n",
    "        database=config['DATABASES_NAME'], user=config['DATABASES_USER'],\n",
    "      password=config['DATABASES_PASSWORD'], host=config['DATABASES_HOST']\n",
    "     )\n",
    "  return conn\n",
    "\n",
    " except Exception as error:\n",
    "  print(error)      \n",
    "  raise error\n",
    "def list_data(sql,params,connection):\n",
    " df=None   \n",
    " with connection.cursor() as cursor:\n",
    "    \n",
    "    if params is None:\n",
    "       cursor.execute(sql)\n",
    "    else:\n",
    "       cursor.execute(sql,params)\n",
    "    \n",
    "    columns = [col[0] for col in cursor.description]\n",
    "    dataList = [dict(zip(columns, row)) for row in cursor.fetchall()]\n",
    "    df = pd.DataFrame(data=dataList) \n",
    " return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bq_table():\n",
    " try:\n",
    "    table=client.get_table(table_id)  # Make an API request.\n",
    "    print(\"Table {} already exists.\".format(table_id))\n",
    "    print(table.schema)\n",
    "    return True\n",
    " except NotFound:\n",
    "    raise Exception(\"Table {} is not found.\".format(table_id))\n",
    "    \n",
    "\n",
    "def insertDataFrameToBQ(df_trasns):\n",
    "    try:\n",
    "        job_config = bigquery.LoadJobConfig(write_disposition=to_bq_mode,)\n",
    "        job = client.load_table_from_dataframe(df_trasns, table_id, job_config=job_config)\n",
    "        try:\n",
    "         job.result()  # Wait for the job to complete.\n",
    "        except ClientError as e:\n",
    "         print(job.errors)\n",
    "\n",
    "        print(\"Total \", len(df_trasns), f\"Imported data to {table_id} on bigquery successfully\")\n",
    "\n",
    "    except BadRequest as e:\n",
    "        print(\"Bigquery Error\\n\")\n",
    "        print(e) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check whether it is the first loading?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def checkFirstLoad():\n",
    "    print(\"If the main table is empty , so the action of each row  must be 'added' on temp table\")\n",
    "    rows_iter   = client.list_rows(main_table_id, max_results=1) \n",
    "    no_main=len(list(rows_iter))\n",
    "    if no_main==0:\n",
    "     isFirstLoad=True\n",
    "     print(f\"This is the first loaing , so there is No DATA in {main_table_id}, we load all rows from {view_name} to import into {table_id} action will be 'added' \")\n",
    "    else:\n",
    "     isFirstLoad=False   \n",
    "    return isFirstLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the main table is empty , so the action of each row  must be 'added' on temp table\n",
      "This is the first loaing , so there is No DATA in pongthorn.SMartDataAnalytics.incident, we load all rows from xyz_incident to import into pongthorn.SMartData_Temp.temp_incident action will be 'added' \n",
      "IsFirstLoad=True for incident\n"
     ]
    }
   ],
   "source": [
    "isFirstLoad=checkFirstLoad()\n",
    "print(f\"IsFirstLoad={isFirstLoad} for {data_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For The next Load\n",
    "* Get data from model log based on condition last_imported and table\n",
    "* Get all actions from log table by selecting unique object_id and setting by doing something as logic\n",
    "* Create  id and action dataframe form filtered rows from log table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_model_log(x_last_imported,x_content_id):\n",
    "    sql_log = f\"\"\"\n",
    "    SELECT object_id, action,TO_CHAR(date_created,'YYYY-MM-DD HH24:MI:SS') as date_created ,changed_data\n",
    "    FROM {log}\n",
    "    WHERE date_created  AT time zone 'utc' >= '{x_last_imported}' AND content_type_id = {x_content_id} \n",
    "    ORDER BY object_id, date_created\n",
    "    \"\"\"\n",
    "    print(sql_log)\n",
    "\n",
    "\n",
    "    # Asia/Bangkok \n",
    "    lf = list_data(sql_log, None, get_postgres_conn())\n",
    "    print(f\"Retrieve all rows after {last_imported}\")\n",
    "    print(lf.info())\n",
    "    return lf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_any_changes_to_collumns_view(dfAction,x_view_name,_x_key_name):\n",
    "    \"\"\"\n",
    "    Check dataframe from log model that contain only changed action to select changed fields on view.\n",
    "    \"\"\"\n",
    "\n",
    "    listACtion=dfAction[\"action\"].unique().tolist()\n",
    "    if len(listACtion)==1 and listACtion[0]=='changed':\n",
    "        print(\"###########################################################\")\n",
    "        print(\"Process dataframe containing only all changed action\")\n",
    "        print(dfAction)\n",
    "        print(\"###########################################################\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_actual_action(lf):\n",
    "    listIDs=lf[\"object_id\"].unique().tolist()\n",
    "    listUpdateData=[]\n",
    "    for id in listIDs:\n",
    "        lfTemp=lf.query(\"object_id==@id\")\n",
    "        print(f\"--------------------{id}---------------------------------\")\n",
    "        print(lfTemp)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # check_any_changes_to_collumns_view(lfTemp,content_id,view_name_id)\n",
    "\n",
    "\n",
    "        first_row = lfTemp.iloc[0]\n",
    "        last_row = lfTemp.iloc[-1]\n",
    "        # print(first_row)\n",
    "        # print(last_row)\n",
    "\n",
    "        if len(lfTemp)==1:\n",
    "            listUpdateData.append([id,first_row[\"action\"]])\n",
    "        else:\n",
    "            if first_row[\"action\"] == \"added\" and last_row[\"action\"] == \"deleted\":\n",
    "                continue\n",
    "            elif first_row[\"action\"] == \"added\" and last_row[\"action\"] != \"deleted\":\n",
    "                listUpdateData.append([id,\"added\"])\n",
    "            else : listUpdateData.append([id,last_row[\"action\"]])\n",
    "\n",
    "    print(\"Convert listUpdate to dataframe\")\n",
    "    dfUpdateData = pd.DataFrame(listUpdateData, columns= ['id', 'action'])\n",
    "    dfUpdateData['id'] = dfUpdateData['id'].astype('int64')\n",
    "    dfUpdateData=dfUpdateData.sort_values(by=\"id\")\n",
    "    dfUpdateData=dfUpdateData.reset_index(drop=True)\n",
    "\n",
    "    return dfUpdateData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if isFirstLoad==False:\n",
    "    listModelLogObjectIDs=[]\n",
    "    dfModelLog=list_model_log(last_imported,content_id)\n",
    "    if dfModelLog.empty==True:\n",
    "        print(\"No row to be imported.\")\n",
    "        exit()\n",
    "    else:\n",
    "       print(\"Get row imported from model log to set action\") \n",
    "       dfModelLog=select_actual_action( dfModelLog)\n",
    "       listModelLogObjectIDs=dfModelLog['id'].tolist()\n",
    "       print(dfModelLog.info())\n",
    "       print(dfModelLog)       \n",
    "       print(listModelLogObjectIDs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load view and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select *  from xyz_incident  where  updated_at AT time zone 'utc' >= '2024-01-01 00:00:00'\n",
      "   incident_id  inventory_id            incident_type service_type  severity  \\\n",
      "0         4302          9594  Network Adapter Failure     Incident     Minor   \n",
      "1         4303         17974         General Incident      Request  Cosmetic   \n",
      "2         4298         12306         General Incident     Incident     Major   \n",
      "3         4299         12306         General Incident     Incident     Major   \n",
      "4         4301         19244       Maintenance System      Request  Cosmetic   \n",
      "5         4304         12305         General Incident     Incident     Major   \n",
      "6         4305         19211                 Software     Incident     Major   \n",
      "7         4300           993                 Software     Incident     Major   \n",
      "\n",
      "        status        open_datetime       close_datetime action  \n",
      "0         Open  2023-12-27 09:55:00  2024-01-03 02:00:00  added  \n",
      "1  In Progress  2024-01-03 09:00:00  2024-01-03 12:00:00  added  \n",
      "2         Open  2023-12-30 12:08:00  2023-12-30 13:00:00  added  \n",
      "3         Open  2023-12-30 18:20:00  2023-12-30 19:00:00  added  \n",
      "4  In Progress  2024-01-02 21:49:00                 None  added  \n",
      "5         Open  2024-01-03 09:44:00  2024-01-03 10:20:00  added  \n",
      "6         Open  2024-01-08 00:00:00                 None  added  \n",
      "7         Open  2024-01-02 13:00:00                 None  added  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8 entries, 0 to 7\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   incident_id     8 non-null      int64 \n",
      " 1   inventory_id    8 non-null      int64 \n",
      " 2   incident_type   8 non-null      object\n",
      " 3   service_type    8 non-null      object\n",
      " 4   severity        8 non-null      object\n",
      " 5   status          8 non-null      object\n",
      " 6   open_datetime   8 non-null      object\n",
      " 7   close_datetime  5 non-null      object\n",
      " 8   action          8 non-null      object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 704.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def retrive_next_data_from_view(x_view,x_id,x_listModelLogObjectIDs):\n",
    "    if len(x_listModelLogObjectIDs)>1:\n",
    "     sql_view=f\"select *  from {x_view}  where {x_id} in {tuple(x_listModelLogObjectIDs)}\"\n",
    "    else:\n",
    "     sql_view=f\"select *  from {x_view}  where {x_id} ={x_listModelLogObjectIDs[0]}\"\n",
    "    \n",
    "    print(sql_view)\n",
    "    df=list_data(sql_view,None,get_postgres_conn())\n",
    "\n",
    "    if df.empty==True:\n",
    "     return df\n",
    "    df=df.drop(columns='updated_at')\n",
    "    return df \n",
    "\n",
    "\n",
    "def retrive_first_data_from_view(x_view,x_last_imported):\n",
    "     sql_view=f\"select *  from {x_view}  where  updated_at AT time zone 'utc' >= '{x_last_imported}'\"\n",
    "     print(sql_view)\n",
    "     df=list_data(sql_view,None,get_postgres_conn())\n",
    "     if df.empty==True:\n",
    "            return df\n",
    "     df=df.drop(columns='updated_at')\n",
    "     df['action']='added'\n",
    "     return df   \n",
    "def retrive_one_row_from_view_to_gen_df_schema(x_view):\n",
    "    sql_view=f\"select *  from {x_view}  limit 1\"\n",
    "    print(sql_view)\n",
    "    df=list_data(sql_view,None,get_postgres_conn())\n",
    "    df=df.drop(columns='updated_at')\n",
    "    return df\n",
    "\n",
    "\n",
    "if isFirstLoad:\n",
    " df=retrive_first_data_from_view(view_name,last_imported)\n",
    " if df.empty==True:\n",
    "    print(\"No row to be imported.\")\n",
    "    exit()\n",
    " else:\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    " df=retrive_next_data_from_view(view_name,view_name_id,listModelLogObjectIDs)  \n",
    " if df.empty==True:\n",
    "    print(\"Due to having deleted items, we will Get schema from {} to create empty dataframe with schema.\")\n",
    "    df=retrive_one_row_from_view_to_gen_df_schema(view_name)\n",
    "    # this id has been included in listModelLogObjectIDs which contain deleted action , so we can use it as schema generation\n",
    "    print(df)\n",
    "\n",
    "    \n",
    "print(df.info())    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transaformation\n",
    "* IF The first load then add actio='Added'\n",
    "* IF The nextload then Merge LogDF and ViewDF and add deleted row \n",
    "  * Get Deleted Items  to Create deleted dataframe by using listDeleted\n",
    "  * If there is one deletd row then  we will merge it to master dataframe\n",
    "* IF the next load has only deleted action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_acutal_action_to_df_at_next(df,dfUpdateData,x_view,x_id):\n",
    "    # merget model log(id and action) to data view\n",
    "    # if  dfUpdateData  contain only deleted action\n",
    "    # we will merge to get datafdame shcema, it can perform inner without have actual data fram view\n",
    "    merged_df = pd.merge(df, dfUpdateData, left_on=view_name_id, right_on='id', how='inner')\n",
    "    merged_df = merged_df.drop(columns=['id'])\n",
    "\n",
    "    listAllAction=dfUpdateData['id'].tolist()\n",
    "    print(f\"List {listAllAction} all action\")\n",
    "    \n",
    "    listSeleted = merged_df[view_name_id].tolist()\n",
    "    print(f\"List  {x_view}   {listSeleted} from {x_view} exluding deleted action\")\n",
    "    \n",
    "    allActionSet = set(listAllAction)\n",
    "    anotherSet = set(listSeleted)\n",
    "    \n",
    "    listDeleted = list(allActionSet.symmetric_difference(anotherSet))\n",
    "    print(f\"List deleted {listDeleted}\")\n",
    "    \n",
    "    # Test List  select by view + List deeleted = List All Action\n",
    "\n",
    "    if len(listDeleted)>0:\n",
    "        print(\"There are some deleted rows\")\n",
    "        dfDeleted=pd.DataFrame(data=listDeleted,columns=[view_name_id])\n",
    "        dfDeleted['action']='deleted'\n",
    "        print(dfDeleted)\n",
    "        merged_df=pd.concat([merged_df,dfDeleted],axis=0)\n",
    "\n",
    "    else:\n",
    "        print(\"No row deleted\")\n",
    "\n",
    "    return merged_df    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   incident_id  inventory_id            incident_type service_type  severity  \\\n",
      "0         4302          9594  Network Adapter Failure     Incident     Minor   \n",
      "1         4303         17974         General Incident      Request  Cosmetic   \n",
      "2         4298         12306         General Incident     Incident     Major   \n",
      "3         4299         12306         General Incident     Incident     Major   \n",
      "4         4301         19244       Maintenance System      Request  Cosmetic   \n",
      "5         4304         12305         General Incident     Incident     Major   \n",
      "6         4305         19211                 Software     Incident     Major   \n",
      "7         4300           993                 Software     Incident     Major   \n",
      "\n",
      "        status        open_datetime       close_datetime action  \n",
      "0         Open  2023-12-27 09:55:00  2024-01-03 02:00:00  added  \n",
      "1  In Progress  2024-01-03 09:00:00  2024-01-03 12:00:00  added  \n",
      "2         Open  2023-12-30 12:08:00  2023-12-30 13:00:00  added  \n",
      "3         Open  2023-12-30 18:20:00  2023-12-30 19:00:00  added  \n",
      "4  In Progress  2024-01-02 21:49:00                 None  added  \n",
      "5         Open  2024-01-03 09:44:00  2024-01-03 10:20:00  added  \n",
      "6         Open  2024-01-08 00:00:00                 None  added  \n",
      "7         Open  2024-01-02 13:00:00                 None  added  \n"
     ]
    }
   ],
   "source": [
    "if isFirstLoad==False:\n",
    " df=add_acutal_action_to_df_at_next(df,dfModelLog,view_name,view_name_id)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last Step :Check duplicate ID & reset index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no duplicate incident_id ID\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8 entries, 0 to 7\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   incident_id     8 non-null      int64 \n",
      " 1   inventory_id    8 non-null      int64 \n",
      " 2   incident_type   8 non-null      object\n",
      " 3   service_type    8 non-null      object\n",
      " 4   severity        8 non-null      object\n",
      " 5   status          8 non-null      object\n",
      " 6   open_datetime   8 non-null      object\n",
      " 7   close_datetime  5 non-null      object\n",
      " 8   action          8 non-null      object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 704.0+ bytes\n",
      "None\n",
      "   incident_id  inventory_id            incident_type service_type  severity  \\\n",
      "0         4302          9594  Network Adapter Failure     Incident     Minor   \n",
      "1         4303         17974         General Incident      Request  Cosmetic   \n",
      "2         4298         12306         General Incident     Incident     Major   \n",
      "3         4299         12306         General Incident     Incident     Major   \n",
      "4         4301         19244       Maintenance System      Request  Cosmetic   \n",
      "5         4304         12305         General Incident     Incident     Major   \n",
      "6         4305         19211                 Software     Incident     Major   \n",
      "7         4300           993                 Software     Incident     Major   \n",
      "\n",
      "        status        open_datetime       close_datetime action  \n",
      "0         Open  2023-12-27 09:55:00  2024-01-03 02:00:00  added  \n",
      "1  In Progress  2024-01-03 09:00:00  2024-01-03 12:00:00  added  \n",
      "2         Open  2023-12-30 12:08:00  2023-12-30 13:00:00  added  \n",
      "3         Open  2023-12-30 18:20:00  2023-12-30 19:00:00  added  \n",
      "4  In Progress  2024-01-02 21:49:00                 None  added  \n",
      "5         Open  2024-01-03 09:44:00  2024-01-03 10:20:00  added  \n",
      "6         Open  2024-01-08 00:00:00                 None  added  \n",
      "7         Open  2024-01-02 13:00:00                 None  added  \n"
     ]
    }
   ],
   "source": [
    "hasDplicateIDs = df[view_name_id].duplicated().any()\n",
    "if  hasDplicateIDs:\n",
    " raise Exception(\"There are some duplicate id on dfUpdateData\")\n",
    "else:\n",
    " print(f\"There is no duplicate {view_name_id} ID\")  \n",
    "\n",
    "\n",
    "# merged_df['imported_at']=dt_imported\n",
    "df=df.reset_index(drop=True  )\n",
    "print(df.info())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert data to BQ data frame & # Run StoreProcedure To Merge Temp&Main and Truncate Transaction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if way=='merge':\n",
    "    print(\"1#Ingest data into Bigquery\")\n",
    "    if get_bq_table():\n",
    "        try:\n",
    "            insertDataFrameToBQ(df)\n",
    "        except Exception as ex:\n",
    "            raise ex\n",
    "            \n",
    "    print(\"2#Run StoreProcedure To Merge Temp&Main and Truncate Transaction.\")\n",
    "    # https://cloud.google.com/bigquery/docs/transactions\n",
    "    sp_id_to_invoke=f\"\"\" CALL `{projectId}.{main_dataset_id}.{sp_name}`() \"\"\"\n",
    "    print(sp_id_to_invoke)    \n",
    "    sp_job = client.query(sp_id_to_invoke)\n",
    "\n",
    "else:\n",
    "    bq_storage_api_path=\"bq_storage_api\"\n",
    "    df.to_csv(f\"{data_name}_{way}.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BQ-Storage-API Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert strng to datetime and microseconds\n"
     ]
    }
   ],
   "source": [
    "print(\"convert strng to datetime and microseconds\")\n",
    "from google.protobuf.timestamp_pb2 import Timestamp\n",
    "def convert_string_to_datetime_timestamp_microseconds (dt_str):\n",
    "    if dt_str is not None:   \n",
    "        dt=datetime.strptime(dt_str,\"%Y-%m-%d %H:%M:%S\")\n",
    "        x_timestamp = Timestamp()\n",
    "        x_timestamp.FromDatetime(dt)\n",
    "        micro_x =x_timestamp.ToMicroseconds()\n",
    "        return micro_x\n",
    "    else:\n",
    "        None\n",
    "#        \n",
    "datetimeCols=[\"open_datetime\",\"close_datetime\"]\n",
    "for d in datetimeCols:\n",
    "    # check whick column contain null value if so, convert float64 to int 32\n",
    "    df[d]=df[d].apply(convert_string_to_datetime_timestamp_microseconds)\n",
    "    df[d] = df[d].fillna(0)\n",
    "    df[d]=df[d].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "timestamp = Timestamp()\n",
    "timestamp.FromDatetime(dt_imported)\n",
    "update_at_micro_timestampe =timestamp.ToMicroseconds()\n",
    "df['update_at']=update_at_micro_timestampe \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change action type\n"
     ]
    }
   ],
   "source": [
    "print(\"change action type\")\n",
    "\n",
    "def change_action_merge_to_bq_storage_api(x):\n",
    "    if x==\"added\" or x==\"changed\":\n",
    "        return  \"UPSERT\"\n",
    "    else:\n",
    "        return \"DELETE\"\n",
    "\n",
    "    \n",
    "df[\"_CHANGE_TYPE\"]=df['action'].apply(change_action_merge_to_bq_storage_api)\n",
    "df=df.drop(columns=['action'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8 entries, 0 to 7\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   incident_id     8 non-null      int64 \n",
      " 1   inventory_id    8 non-null      int64 \n",
      " 2   incident_type   8 non-null      object\n",
      " 3   service_type    8 non-null      object\n",
      " 4   severity        8 non-null      object\n",
      " 5   status          8 non-null      object\n",
      " 6   open_datetime   8 non-null      Int64 \n",
      " 7   close_datetime  8 non-null      Int64 \n",
      " 8   update_at       8 non-null      int64 \n",
      " 9   _CHANGE_TYPE    8 non-null      object\n",
      "dtypes: Int64(2), int64(3), object(5)\n",
      "memory usage: 784.0+ bytes\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>incident_id</th>\n",
       "      <th>inventory_id</th>\n",
       "      <th>incident_type</th>\n",
       "      <th>service_type</th>\n",
       "      <th>severity</th>\n",
       "      <th>status</th>\n",
       "      <th>open_datetime</th>\n",
       "      <th>close_datetime</th>\n",
       "      <th>update_at</th>\n",
       "      <th>_CHANGE_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4299</td>\n",
       "      <td>12306</td>\n",
       "      <td>General Incident</td>\n",
       "      <td>Incident</td>\n",
       "      <td>Major</td>\n",
       "      <td>Open</td>\n",
       "      <td>1703960400000000</td>\n",
       "      <td>1703962800000000</td>\n",
       "      <td>1704969356000000</td>\n",
       "      <td>UPSERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4301</td>\n",
       "      <td>19244</td>\n",
       "      <td>Maintenance System</td>\n",
       "      <td>Request</td>\n",
       "      <td>Cosmetic</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>1704232140000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1704969356000000</td>\n",
       "      <td>UPSERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4304</td>\n",
       "      <td>12305</td>\n",
       "      <td>General Incident</td>\n",
       "      <td>Incident</td>\n",
       "      <td>Major</td>\n",
       "      <td>Open</td>\n",
       "      <td>1704275040000000</td>\n",
       "      <td>1704277200000000</td>\n",
       "      <td>1704969356000000</td>\n",
       "      <td>UPSERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4305</td>\n",
       "      <td>19211</td>\n",
       "      <td>Software</td>\n",
       "      <td>Incident</td>\n",
       "      <td>Major</td>\n",
       "      <td>Open</td>\n",
       "      <td>1704672000000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1704969356000000</td>\n",
       "      <td>UPSERT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4300</td>\n",
       "      <td>993</td>\n",
       "      <td>Software</td>\n",
       "      <td>Incident</td>\n",
       "      <td>Major</td>\n",
       "      <td>Open</td>\n",
       "      <td>1704200400000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1704969356000000</td>\n",
       "      <td>UPSERT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   incident_id  inventory_id       incident_type service_type  severity  \\\n",
       "3         4299         12306    General Incident     Incident     Major   \n",
       "4         4301         19244  Maintenance System      Request  Cosmetic   \n",
       "5         4304         12305    General Incident     Incident     Major   \n",
       "6         4305         19211            Software     Incident     Major   \n",
       "7         4300           993            Software     Incident     Major   \n",
       "\n",
       "        status     open_datetime    close_datetime         update_at  \\\n",
       "3         Open  1703960400000000  1703962800000000  1704969356000000   \n",
       "4  In Progress  1704232140000000                 0  1704969356000000   \n",
       "5         Open  1704275040000000  1704277200000000  1704969356000000   \n",
       "6         Open  1704672000000000                 0  1704969356000000   \n",
       "7         Open  1704200400000000                 0  1704969356000000   \n",
       "\n",
       "  _CHANGE_TYPE  \n",
       "3       UPSERT  \n",
       "4       UPSERT  \n",
       "5       UPSERT  \n",
       "6       UPSERT  \n",
       "7       UPSERT  "
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.info())\n",
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df.to_csv(f\"{data_name}_{way}.csv\",index=False)\n",
    "json_file=\"incident.json\"\n",
    "json_file_path=os.path.join(bq_storage_api_path,json_file)\n",
    "\n",
    "json_incident_data = json.loads(df.to_json(orient = 'records'))\n",
    "with open(json_file_path, \"w\") as outfile:\n",
    "    json.dump(json_incident_data, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# Update New Recenet Update to file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConfigUpdater [\n",
       "    <Section: 'metadata' [\n",
       "        <Option: xyz_incident = '2024-01-11 08:34:17'>\n",
       "    ]>\n",
       "]>"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updater[\"metadata\"][view_name].value=dt_imported.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "updater.update_file() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-11 08:34:29.591782+00:00\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now(timezone.utc) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
