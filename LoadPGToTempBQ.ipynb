{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imported Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import psycopg2.extras as extras\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime,timezone\n",
    "import time\n",
    "from dateutil import tz\n",
    "\n",
    "import os\n",
    "import sys \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.exceptions import NotFound\n",
    "from google.api_core.exceptions import BadRequest\n",
    "import os\n",
    "import sys \n",
    "import shutil\n",
    "\n",
    "import CheckDataCons_DB_BQ as check_data  \n",
    "\n",
    "import sqlite3\n",
    "\n",
    "from configupdater import ConfigUpdater\n",
    "# pip install ConfigUpdater\n",
    "\n",
    "from dotenv import dotenv_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "is_py=False\n",
    "check_consistency=False\n",
    "time_wait_for_bq=30\n",
    "view_name = \"xyz_incident\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View name to load to BQ :xyz_incident\n"
     ]
    }
   ],
   "source": [
    "isFirstLoad=False\n",
    "                             \n",
    "\n",
    "if is_py:\n",
    "    press_Y=''\n",
    "    ok=False\n",
    "\n",
    "    if len(sys.argv) > 1:\n",
    "        view_name=sys.argv[1]\n",
    "    else:\n",
    "        print(\"Enter the following input: \")\n",
    "        view_name = input(\"View Table Name : \")\n",
    "print(f\"View name to load to BQ :{view_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imported date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTC: 2024-01-17 18:02:49 For This Import\n"
     ]
    }
   ],
   "source": [
    "dt_imported=datetime.now(timezone.utc) # utc\n",
    "dt_imported=datetime.strptime(dt_imported.strftime(\"%Y-%m-%d %H:%M:%S\"),\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"UTC: {dt_imported} For This Import\")\n",
    "\n",
    "str_dt_imported=dt_imported.strftime(\"%Y-%m-%d %H:%M:%S\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Configuration File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".env\n",
      "cfg_last_import\n"
     ]
    }
   ],
   "source": [
    "# Test config,env file and key to be used ,all of used key  are existing.\n",
    "cfg_path=\"cfg_last_import\"\n",
    "env_path='.env'\n",
    "\n",
    "updater = ConfigUpdater()\n",
    "updater.read(os.path.join(cfg_path,f\"{view_name}.cfg\"))\n",
    "\n",
    "config = dotenv_values(dotenv_path=env_path)\n",
    "\n",
    "data_base_file=\"etl_web_admin/bq_cdc_etl_transaction.db\"\n",
    "\n",
    "print(env_path)\n",
    "print(cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log = \"models_logging_change\"\n",
    "data_name=view_name.replace(\"pmr_\",\"\").replace(\"xyz_\",\"\")\n",
    "sp_name=f\"merge_{data_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sqlite3.register_adapter(np.int64, lambda val: int(val))\n",
    "sqlite3.register_adapter(np.int32, lambda val: int(val))\n",
    "\n",
    "\n",
    "def list_data_sqlite(sql):\n",
    "    conn = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "    print(sql)\n",
    "    df_item=pd.read_sql_query(sql, conn)\n",
    "    return df_item\n",
    "\n",
    "def addETLTrans(recordList):\n",
    "    try:\n",
    "        sqliteConnection = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "        cursor = sqliteConnection.cursor()\n",
    "        sqlite_insert_query = \"\"\"\n",
    "        INSERT INTO etl_transaction\n",
    "        (trans_datetime, view_source_id,no_rows,is_consistent,is_complete)  \n",
    "        VALUES (?,?,?,?,?);\n",
    "         \"\"\"\n",
    "        cursor.executemany(sqlite_insert_query, recordList)\n",
    "        print(\"Done ETL Trasaction\")\n",
    "        sqliteConnection.commit()\n",
    "        cursor.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Failed to insert etl_transaction table\", str(e))\n",
    "    finally:\n",
    "        if sqliteConnection:\n",
    "            sqliteConnection.close()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get View Source  to set configuration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from view_source where name='xyz_incident' limit 1\n",
      "id                                                                           5\n",
      "name                                                              xyz_incident\n",
      "load_type                                                       bq-storage-api\n",
      "app_conten_type_id                                                          18\n",
      "app_key_name                                                       incident_id\n",
      "app_changed_field_mapping    inventory_id,\\r\\nincident_type_id,\\r\\nservice_...\n",
      "app_fk_name_list                                                  inventory_id\n",
      "app_datetime_field_list                                                   None\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def get_view_source(name):\n",
    "    sql=f\"select * from view_source where name='{name}' limit 1\"\n",
    "    dfView=list_data_sqlite(sql)\n",
    "    if dfView.empty==False:\n",
    "       view_source=dfView.iloc[0,:]\n",
    "    else:\n",
    "        error=f\"Not found {view_name} view\"\n",
    "        raise Exception(error)\n",
    "    return view_source\n",
    "view_source= get_view_source(view_name)\n",
    "print(view_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoadyType:bq-storage-api # ContentyTypeID:18 # KeyName:incident_id # SP:merge_incident\n",
      "['inventory_id', 'incident_type_id', 'service_type_id', 'incident_severity_id', 'incident_status_id', 'incident_datetime', 'incident_close_datetime']\n",
      "['inventory_id', 'incident_id']\n"
     ]
    }
   ],
   "source": [
    "admin_view_id=view_source['id']\n",
    "content_id=view_source['app_conten_type_id']\n",
    "view_name_id=view_source['app_key_name']\n",
    "\n",
    "\n",
    "changed_field_mapping=view_source['app_changed_field_mapping'].strip().split(\",\")\n",
    "changed_field_mapping = [ x.replace(\" \", \"\").replace(\"\\r\", \"\").replace(\"\\n\", \"\") for x  in changed_field_mapping] \n",
    "\n",
    "way=view_source['load_type'] # 1=\"merge\"  or \"bq-storage-api\"\n",
    "\n",
    "pk_fk_list=[]\n",
    "if view_source['app_fk_name_list'] is not None:\n",
    "    pk_fk_list=view_source['app_fk_name_list'].strip().split(\",\")\n",
    "    pk_fk_list= [ x.replace(\" \", \"\").replace(\"\\r\", \"\").replace(\"\\n\", \"\") for x  in  pk_fk_list] \n",
    "pk_fk_list.append(view_name_id)\n",
    "\n",
    "print(f\"LoadyType:{way} # ContentyTypeID:{content_id} # KeyName:{view_name_id} # SP:{sp_name}\")\n",
    "print(changed_field_mapping)\n",
    "print(pk_fk_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigQuery Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pongthorn.SMartData_Temp.temp_incident\n",
      "pongthorn.SMartDataAnalytics.incident\n"
     ]
    }
   ],
   "source": [
    "# Test exsitng project dataset and table anme\n",
    "\n",
    "projectId=config['PROJECT_ID']  # smart-data-ml  or kku-intern-dataai or ponthorn\n",
    "credential_file=config['PROJECT_CREDENTIAL_FILE']\n",
    "# C:\\Windows\\smart-data-ml-91b6f6204773.json\n",
    "# C:\\Windows\\kku-intern-dataai-a5449aee8483.json\n",
    "# C:\\Windows\\pongthorn-5decdc5124f5.json\n",
    "\n",
    "\n",
    "dataset_id=config['TEMP_DATASET'] # 'SMartData_Temp'  'PMReport_Temp'\n",
    "main_dataset_id=config['MAIN_DATASET']  # ='SMartDataAnalytics'  'PMReport_Main'\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(credential_file)\n",
    "\n",
    "table_name=f\"temp_{data_name}\" #can change in (\"name\") to temp table\n",
    "table_id = f\"{projectId}.{dataset_id}.{table_name}\"\n",
    "print(table_id)\n",
    "\n",
    "\n",
    "main_table_name=data_name\n",
    "main_table_id = f\"{projectId}.{main_dataset_id}.{main_table_name}\"\n",
    "print(main_table_id)\n",
    "\n",
    "# https://cloud.google.com/bigquery/docs/reference/rest/v2/Job\n",
    "to_bq_mode=\"WRITE_EMPTY\"\n",
    "\n",
    "\n",
    "client = bigquery.Client(credentials= credentials,project=projectId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check configuration parameter validation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Last Import to retrive data after that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incident - UTC:2024-01-17 14:05:03  Of Last Import\n"
     ]
    }
   ],
   "source": [
    "last_imported=datetime.strptime(updater[\"metadata\"][view_name].value,\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"{data_name} - UTC:{last_imported}  Of Last Import\")\n",
    "\n",
    "# local_zone = tz.tzlocal()\n",
    "# last_imported = last_imported.astimezone(local_zone)\n",
    "# print(f\"Local Asia/Bangkok:{last_imported}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postgres &BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_postgres_conn():\n",
    " try:\n",
    "  conn = psycopg2.connect(\n",
    "        database=config['DATABASES_NAME'], user=config['DATABASES_USER'],\n",
    "      password=config['DATABASES_PASSWORD'], host=config['DATABASES_HOST']\n",
    "     )\n",
    "  return conn\n",
    "\n",
    " except Exception as error:\n",
    "  print(error)      \n",
    "  raise error\n",
    "def list_data(sql,params,connection):\n",
    " df=None   \n",
    " with connection.cursor() as cursor:\n",
    "    \n",
    "    if params is None:\n",
    "       cursor.execute(sql)\n",
    "    else:\n",
    "       cursor.execute(sql,params)\n",
    "    \n",
    "    columns = [col[0] for col in cursor.description]\n",
    "    dataList = [dict(zip(columns, row)) for row in cursor.fetchall()]\n",
    "    df = pd.DataFrame(data=dataList) \n",
    " return df \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bq_table():\n",
    " try:\n",
    "    table=client.get_table(table_id)  # Make an API request.\n",
    "    print(\"Table {} already exists.\".format(table_id))\n",
    "    print(table.schema)\n",
    "    return True\n",
    " except NotFound:\n",
    "    raise Exception(\"Table {} is not found.\".format(table_id))\n",
    "    \n",
    "\n",
    "def insertDataFrameToBQ(df_trasns):\n",
    "    try:\n",
    "        job_config = bigquery.LoadJobConfig(write_disposition=to_bq_mode,)\n",
    "        job = client.load_table_from_dataframe(df_trasns, table_id, job_config=job_config)\n",
    "        try:\n",
    "         job.result()  # Wait for the job to complete.\n",
    "        except ClientError as e:\n",
    "         print(job.errors)\n",
    "\n",
    "        print(\"Total \", len(df_trasns), f\"Imported data to {table_id} on bigquery successfully\")\n",
    "\n",
    "    except BadRequest as e:\n",
    "        print(\"Bigquery Error\\n\")\n",
    "        print(e) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Data Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_check_consistency():\n",
    "    check_result=True\n",
    "    if check_consistency:\n",
    "         print(\"Wait in a while for biqguery to update\")\n",
    "         time.sleep(time_wait_for_bq)\n",
    "         print(\"Check data consistency betwwen database and bigquery\")\n",
    "         result=check_data.check_data_consistency_db_bq(view_name)\n",
    "         if result:\n",
    "            print(\"if result=True , view csv file in check_db_bq  data_consistence_check\")  \n",
    "            print(\"send email to admin to investigate somthing wrong.\")\n",
    "            check_result=False\n",
    "         else:\n",
    "            print(f\"Data has been consistent between {config['DATABASES_NAME']} and {main_table_id}\")\n",
    "    else:\n",
    "        print(\"Disable checking data consistency feature.\")\n",
    "            \n",
    "    return int(check_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check whether it is the first loading?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def checkFirstLoad():\n",
    "    print(\"If the main table is empty , so the action of each row  must be 'added' on temp table\")\n",
    "    rows_iter   = client.list_rows(main_table_id, max_results=1) \n",
    "    no_main=len(list(rows_iter))\n",
    "    if no_main==0:\n",
    "     isFirstLoad=True\n",
    "     print(f\"This is the first loaing , so there is No DATA in {main_table_id}, we load all rows from {view_name} to import into {table_id} action will be 'added' \")\n",
    "    else:\n",
    "     isFirstLoad=False   \n",
    "    return isFirstLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the main table is empty , so the action of each row  must be 'added' on temp table\n",
      "IsFirstLoad=False for incident\n"
     ]
    }
   ],
   "source": [
    "isFirstLoad=checkFirstLoad()\n",
    "print(f\"IsFirstLoad={isFirstLoad} for {data_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For The next Load\n",
    "* Get data from model log based on condition last_imported and table\n",
    "* Get all actions from log table by selecting unique object_id and setting by doing something as logic\n",
    "* Create  id and action dataframe form filtered rows from log table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_model_log(x_last_imported,x_content_id):\n",
    "    sql_log = f\"\"\"\n",
    "    SELECT object_id, action,TO_CHAR(date_created,'YYYY-MM-DD HH24:MI:SS') as date_created ,changed_data\n",
    "    FROM {log}\n",
    "    WHERE date_created  AT time zone 'utc' >= '{x_last_imported}' AND content_type_id = {x_content_id} \n",
    "    ORDER BY object_id, date_created\n",
    "    \"\"\"\n",
    "    print(sql_log)\n",
    "\n",
    "\n",
    "    # Asia/Bangkok \n",
    "    lf = list_data(sql_log, None, get_postgres_conn())\n",
    "    print(f\"Retrieve all rows after {last_imported}\")\n",
    "    print(lf.info())\n",
    "    return lf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Change in Mappping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def findChangeInListMapping(changed_data):\n",
    "    # print(type(changed_data))\n",
    "    # print(changed_data)\n",
    "    x=False\n",
    "    for key in changed_data.keys():\n",
    "        # print(key)\n",
    "        if key in changed_field_mapping :\n",
    "            print(f\"{key} in {changed_field_mapping}\")\n",
    "            x= True\n",
    "\n",
    "    return x\n",
    "    \n",
    "\n",
    "def check_no_changes_to_columns_view_only_changed_action(dfAction,x_view_name,_x_key_name):\n",
    "    \"\"\"\n",
    "    Check dataframe from log model that contain only changed action to select changed fields on view.\n",
    "    Gather id no any changes based on  changed_field_mapping to get rid of it from list to import to BQ\n",
    "    \"\"\"\n",
    "\n",
    "    listACtion=dfAction[\"action\"].unique().tolist()\n",
    "    if len(listACtion)==1 and listACtion[0]=='changed':\n",
    "        print(\"#######################Find Some Changes#############################\")\n",
    "        print(\"Process dataframe containing only all changed action\")\n",
    "        dfAction['x']=dfAction['changed_data'].apply(findChangeInListMapping)\n",
    "        print(dfAction[['object_id','x','changed_data']])\n",
    "        \n",
    "        any_rows_match = dfAction['x'] ==True\n",
    "        match_x=any_rows_match.any()\n",
    "        print(\"Check whether at least one row in a DataFrame matches a specific criteria\")\n",
    "        print(match_x)\n",
    "        # there is at least one change in mapping changed_field_mapping : return false\n",
    "        if match_x:\n",
    "            return False\n",
    "        # there is no any change in mapping changed_field_mapping : return true  \n",
    "        else: # return to caller for deleteing from list\n",
    "            return True\n",
    "        print(\"#####################################################################\")\n",
    "    else:\n",
    "        return False\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "listForRemove=[]\n",
    "def select_actual_action(lf):\n",
    "    listIDs=lf[\"object_id\"].unique().tolist()\n",
    "    listUpdateData=[]\n",
    "    for id in listIDs:\n",
    "        lfTemp=lf.query(\"object_id==@id\")\n",
    "        print(f\"--------------------{id}---------------------------------\")\n",
    "        print(lfTemp)\n",
    "        print(f\"--------------------end---------------------------------\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        x=check_no_changes_to_columns_view_only_changed_action(lfTemp,view_name,view_name_id)\n",
    "        if x==True:\n",
    "           print(f\"RemoveID {id}\") \n",
    "           listForRemove.append(id) \n",
    "\n",
    "\n",
    "        first_row = lfTemp.iloc[0]\n",
    "        last_row = lfTemp.iloc[-1]\n",
    "        # print(first_row)\n",
    "        # print(last_row)\n",
    "\n",
    "        if len(lfTemp)==1:\n",
    "            listUpdateData.append([id,first_row[\"action\"]])\n",
    "        else:\n",
    "            if first_row[\"action\"] == \"added\" and last_row[\"action\"] == \"deleted\":\n",
    "                continue\n",
    "            elif first_row[\"action\"] == \"added\" and last_row[\"action\"] != \"deleted\":\n",
    "                listUpdateData.append([id,\"added\"])\n",
    "            else : listUpdateData.append([id,last_row[\"action\"]])\n",
    "\n",
    "    print(\"Convert listUpdate to dataframe\")\n",
    "    dfUpdateData = pd.DataFrame(listUpdateData, columns= ['id', 'action'])\n",
    "    dfUpdateData['id'] = dfUpdateData['id'].astype('int64')\n",
    "    dfUpdateData=dfUpdateData.sort_values(by=\"id\")\n",
    "    dfUpdateData=dfUpdateData.reset_index(drop=True)\n",
    "\n",
    "    return dfUpdateData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    SELECT object_id, action,TO_CHAR(date_created,'YYYY-MM-DD HH24:MI:SS') as date_created ,changed_data\n",
      "    FROM models_logging_change\n",
      "    WHERE date_created  AT time zone 'utc' >= '2024-01-17 14:05:03' AND content_type_id = 18 \n",
      "    ORDER BY object_id, date_created\n",
      "    \n",
      "Retrieve all rows after 2024-01-17 14:05:03\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   object_id     5 non-null      object\n",
      " 1   action        5 non-null      object\n",
      " 2   date_created  5 non-null      object\n",
      " 3   changed_data  5 non-null      object\n",
      "dtypes: object(4)\n",
      "memory usage: 288.0+ bytes\n",
      "None\n",
      "Get row imported from model log to set action\n",
      "--------------------4097---------------------------------\n",
      "  object_id   action         date_created  \\\n",
      "0      4097  deleted  2024-01-17 21:11:32   \n",
      "\n",
      "                                        changed_data  \n",
      "0  {'id': {'old': 4097}, 'updated_at': {'old': '2...  \n",
      "--------------------end---------------------------------\n",
      "--------------------4274---------------------------------\n",
      "  object_id   action         date_created  \\\n",
      "1      4274  changed  2024-01-17 21:18:51   \n",
      "\n",
      "                                        changed_data  \n",
      "1  {'updated_at': {'new': '2024-01-17T14:18:50.59...  \n",
      "--------------------end---------------------------------\n",
      "#######################Find Some Changes#############################\n",
      "Process dataframe containing only all changed action\n",
      "  object_id      x                                       changed_data\n",
      "1      4274  False  {'updated_at': {'new': '2024-01-17T14:18:50.59...\n",
      "Check whether at least one row in a DataFrame matches a specific criteria\n",
      "False\n",
      "RemoveID 4274\n",
      "--------------------4372---------------------------------\n",
      "  object_id   action         date_created  \\\n",
      "2      4372  changed  2024-01-17 21:14:04   \n",
      "\n",
      "                                        changed_data  \n",
      "2  {'updated_at': {'new': '2024-01-17T14:14:03.60...  \n",
      "--------------------end---------------------------------\n",
      "#######################Find Some Changes#############################\n",
      "Process dataframe containing only all changed action\n",
      "incident_status_id in ['inventory_id', 'incident_type_id', 'service_type_id', 'incident_severity_id', 'incident_status_id', 'incident_datetime', 'incident_close_datetime']\n",
      "incident_close_datetime in ['inventory_id', 'incident_type_id', 'service_type_id', 'incident_severity_id', 'incident_status_id', 'incident_datetime', 'incident_close_datetime']\n",
      "  object_id     x                                       changed_data\n",
      "2      4372  True  {'updated_at': {'new': '2024-01-17T14:14:03.60...\n",
      "Check whether at least one row in a DataFrame matches a specific criteria\n",
      "True\n",
      "--------------------4384---------------------------------\n",
      "  object_id   action         date_created  \\\n",
      "3      4384    added  2024-01-17 21:09:31   \n",
      "4      4384  changed  2024-01-17 21:13:39   \n",
      "\n",
      "                                        changed_data  \n",
      "3  {'id': {'new': 4384, 'old': None}, 'updated_at...  \n",
      "4  {'updated_at': {'new': '2024-01-17T14:13:38.32...  \n",
      "--------------------end---------------------------------\n",
      "Convert listUpdate to dataframe\n",
      "Remove these Ids from dfModelLog : [4274]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3 entries, 0 to 3\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      3 non-null      int64 \n",
      " 1   action  3 non-null      object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 72.0+ bytes\n",
      "None\n",
      "     id   action\n",
      "0  4097  deleted\n",
      "2  4372  changed\n",
      "3  4384    added\n",
      "[4097, 4372, 4384]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pongthsa\\AppData\\Local\\Temp\\ipykernel_27884\\1610703201.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfAction['x']=dfAction['changed_data'].apply(findChangeInListMapping)\n",
      "C:\\Users\\pongthsa\\AppData\\Local\\Temp\\ipykernel_27884\\1610703201.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfAction['x']=dfAction['changed_data'].apply(findChangeInListMapping)\n"
     ]
    }
   ],
   "source": [
    "if isFirstLoad==False:\n",
    "    listModelLogObjectIDs=[]\n",
    "    dfModelLog=list_model_log(last_imported,content_id)\n",
    "    if dfModelLog.empty==True:\n",
    "\n",
    "        dfTran=pd.DataFrame(data={\n",
    "        \"trans_datetime\":[str_dt_imported],\"view_source_id\":[admin_view_id],\n",
    "        \"no_rows\":[0],\"is_consistent\":[do_check_consistency()],\"is_complete\":[1]\n",
    "        } )\n",
    "        addETLTrans(dfTran.to_records(index=False) )\n",
    "        print(\"No row to be imported.\")\n",
    "        exit()\n",
    "    else:\n",
    "        print(\"Get row imported from model log to set action\") \n",
    "        dfModelLog=select_actual_action( dfModelLog)\n",
    "        listForRemove=[int(id) for id in listForRemove ]\n",
    "        print(f\"Remove these Ids from dfModelLog : {listForRemove}\")\n",
    "        dfModelLog=dfModelLog.query(\"id not in @listForRemove\")\n",
    "        listModelLogObjectIDs=dfModelLog['id'].tolist()\n",
    "\n",
    "        print(dfModelLog.info())\n",
    "        print(dfModelLog)       \n",
    "        print(listModelLogObjectIDs) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load view and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select *  from xyz_incident  where incident_id in (4097, 4372, 4384)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2 entries, 0 to 1\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   incident_id     2 non-null      int64 \n",
      " 1   inventory_id    2 non-null      int64 \n",
      " 2   incident_type   2 non-null      object\n",
      " 3   service_type    2 non-null      object\n",
      " 4   severity        2 non-null      object\n",
      " 5   status          2 non-null      object\n",
      " 6   open_datetime   2 non-null      object\n",
      " 7   close_datetime  2 non-null      object\n",
      "dtypes: int64(2), object(6)\n",
      "memory usage: 256.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def retrive_next_data_from_view(x_view,x_id,x_listModelLogObjectIDs):\n",
    "    if len(x_listModelLogObjectIDs)>1:\n",
    "     sql_view=f\"select *  from {x_view}  where {x_id} in {tuple(x_listModelLogObjectIDs)}\"\n",
    "    else:\n",
    "     sql_view=f\"select *  from {x_view}  where {x_id} ={x_listModelLogObjectIDs[0]}\"\n",
    "    \n",
    "    print(sql_view)\n",
    "    df=list_data(sql_view,None,get_postgres_conn())\n",
    "\n",
    "    if df.empty==True:\n",
    "     return df\n",
    "    df=df.drop(columns='updated_at')\n",
    "    return df \n",
    "\n",
    "\n",
    "def retrive_first_data_from_view(x_view,x_last_imported):\n",
    "     sql_view=f\"select *  from {x_view}  where  updated_at AT time zone 'utc' >= '{x_last_imported}'\"\n",
    "     print(sql_view)\n",
    "     df=list_data(sql_view,None,get_postgres_conn())\n",
    "     if df.empty==True:\n",
    "            return df\n",
    "     df=df.drop(columns='updated_at')\n",
    "     df['action']='added'\n",
    "     return df   \n",
    "def retrive_one_row_from_view_to_gen_df_schema(x_view):\n",
    "    sql_view=f\"select *  from {x_view}  limit 1\"\n",
    "    print(sql_view)\n",
    "    df=list_data(sql_view,None,get_postgres_conn())\n",
    "    df=df.drop(columns='updated_at')\n",
    "    return df\n",
    "\n",
    "\n",
    "if isFirstLoad:\n",
    " df=retrive_first_data_from_view(view_name,last_imported)\n",
    " if df.empty==True:\n",
    "    # create dataframe and addETLTrans 0 row    \n",
    "    print(\"No row to be imported.\")\n",
    "    exit()\n",
    " else:\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    " df=retrive_next_data_from_view(view_name,view_name_id,listModelLogObjectIDs)  \n",
    " if df.empty==True:\n",
    "    print(\"Due to having deleted items, we will Get schema from {} to create empty dataframe with schema.\")\n",
    "    df=retrive_one_row_from_view_to_gen_df_schema(view_name)\n",
    "    # this id has been included in listModelLogObjectIDs which contain deleted action , so we can use it as schema generation\n",
    "    print(df)\n",
    "\n",
    "    \n",
    "print(df.info())    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transaformation\n",
    "* IF The first load then add actio='Added'\n",
    "* IF The nextload then Merge LogDF and ViewDF and add deleted row \n",
    "  * Get Deleted Items  to Create deleted dataframe by using listDeleted\n",
    "  * If there is one deletd row then  we will merge it to master dataframe\n",
    "* IF the next load has only deleted action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_acutal_action_to_df_at_next(df,dfUpdateData,x_view,x_id):\n",
    "    # merget model log(id and action) to data view\n",
    "    # if  dfUpdateData  contain only deleted action\n",
    "    # we will merge to get datafdame shcema, it can perform inner without have actual data fram view\n",
    "    merged_df = pd.merge(df, dfUpdateData, left_on=view_name_id, right_on='id', how='inner')\n",
    "    merged_df = merged_df.drop(columns=['id'])\n",
    "\n",
    "    listAllAction=dfUpdateData['id'].tolist()\n",
    "    print(f\"List {listAllAction} all action\")\n",
    "    \n",
    "    listSeleted = merged_df[view_name_id].tolist()\n",
    "    print(f\"List  {x_view}   {listSeleted} from {x_view} exluding deleted action\")\n",
    "    \n",
    "    allActionSet = set(listAllAction)\n",
    "    anotherSet = set(listSeleted)\n",
    "    \n",
    "    listDeleted = list(allActionSet.symmetric_difference(anotherSet))\n",
    "    print(f\"List deleted {listDeleted}\")\n",
    "    \n",
    "    # Test List  select by view + List deeleted = List All Action\n",
    "\n",
    "    if len(listDeleted)>0:\n",
    "        print(\"There are some deleted rows\")\n",
    "        dfDeleted=pd.DataFrame(data=listDeleted,columns=[view_name_id])\n",
    "        dfDeleted['action']='deleted'\n",
    "        print(dfDeleted)\n",
    "        merged_df=pd.concat([merged_df,dfDeleted],axis=0)\n",
    "\n",
    "    else:\n",
    "        print(\"No row deleted\")\n",
    "\n",
    "    return merged_df    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List [4097, 4372, 4384] all action\n",
      "List  xyz_incident   [4372, 4384] from xyz_incident exluding deleted action\n",
      "List deleted [4097]\n",
      "There are some deleted rows\n",
      "   incident_id   action\n",
      "0         4097  deleted\n",
      "   incident_id  inventory_id            incident_type service_type  severity  \\\n",
      "0         4372       17269.0  Controller/Node Failure     Incident     Major   \n",
      "1         4384       19610.0                 Software     Incident  Critical   \n",
      "0         4097           NaN                      NaN          NaN       NaN   \n",
      "\n",
      "   status     open_datetime    close_datetime   action  \n",
      "0  Closed  2024-01-15 06:16  2024-01-17 14:13  changed  \n",
      "1  Closed  2024-01-17 14:08  2024-01-17 14:13    added  \n",
      "0     NaN               NaN               NaN  deleted  \n"
     ]
    }
   ],
   "source": [
    "if isFirstLoad==False:\n",
    " df=add_acutal_action_to_df_at_next(df,dfModelLog,view_name,view_name_id)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last Step :Check duplicate ID & reset index & convert all pk&fk to int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no duplicate incident_id ID\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3 entries, 0 to 2\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   incident_id     3 non-null      Int64 \n",
      " 1   inventory_id    2 non-null      Int64 \n",
      " 2   incident_type   2 non-null      object\n",
      " 3   service_type    2 non-null      object\n",
      " 4   severity        2 non-null      object\n",
      " 5   status          2 non-null      object\n",
      " 6   open_datetime   2 non-null      object\n",
      " 7   close_datetime  2 non-null      object\n",
      " 8   action          3 non-null      object\n",
      "dtypes: Int64(2), object(7)\n",
      "memory usage: 350.0+ bytes\n",
      "None\n",
      "   incident_id  inventory_id            incident_type service_type  severity  \\\n",
      "0         4372         17269  Controller/Node Failure     Incident     Major   \n",
      "1         4384         19610                 Software     Incident  Critical   \n",
      "2         4097          <NA>                      NaN          NaN       NaN   \n",
      "\n",
      "   status     open_datetime    close_datetime   action  \n",
      "0  Closed  2024-01-15 06:16  2024-01-17 14:13  changed  \n",
      "1  Closed  2024-01-17 14:08  2024-01-17 14:13    added  \n",
      "2     NaN               NaN               NaN  deleted  \n"
     ]
    }
   ],
   "source": [
    "hasDplicateIDs = df[view_name_id].duplicated().any()\n",
    "if  hasDplicateIDs:\n",
    " raise Exception(\"There are some duplicate id on dfUpdateData\")\n",
    "else:\n",
    " print(f\"There is no duplicate {view_name_id} ID\")  \n",
    "\n",
    "if len(pk_fk_list)>0:\n",
    " df[pk_fk_list] = df[pk_fk_list].astype('Int64')\n",
    "\n",
    "# merged_df['imported_at']=dt_imported\n",
    "df=df.reset_index(drop=True  )\n",
    "print(df.info())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert data to BQ data frame & # Run StoreProcedure To Merge Temp&Main and Truncate Transaction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if way=='merge':\n",
    "    print(\"1#Ingest data into Bigquery\")\n",
    "    if get_bq_table():\n",
    "        try:\n",
    "            insertDataFrameToBQ(df)\n",
    "        except Exception as ex:\n",
    "            raise ex\n",
    "            \n",
    "    print(\"2#Run StoreProcedure To Merge Temp&Main and Truncate Transaction.\")\n",
    "    # https://cloud.google.com/bigquery/docs/transactions\n",
    "    sp_id_to_invoke=f\"\"\" CALL `{projectId}.{main_dataset_id}.{sp_name}`() \"\"\"\n",
    "    print(sp_id_to_invoke)    \n",
    "    sp_job = client.query(sp_id_to_invoke)\n",
    "\n",
    "else:\n",
    "    df.to_csv(f\"bq_storage_api/{data_name}_{way}.csv\",index=False)\n",
    "    # invoke ingest_data_bq_storage_api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# Update New Recenet Update to file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConfigUpdater [\n",
       "    <Section: 'metadata' [\n",
       "        <Option: pmr_pm_plan = '2024-01-17 17:57:12'>\n",
       "    ]>\n",
       "]>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updater[\"metadata\"][view_name].value=dt_imported.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "updater.update_file() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-17 17:58:51.052539+00:00\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now(timezone.utc) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add ETL transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add ETLTrans n-row as dataframe\n",
      "Disable checking data consistency feature.\n",
      "Done ETL Trasaction\n"
     ]
    }
   ],
   "source": [
    "print(\"Add ETLTrans n-row as dataframe\")   \n",
    "\n",
    "dfTran=pd.DataFrame(data={\n",
    "\"trans_datetime\":[str_dt_imported],\"view_source_id\":[admin_view_id],\n",
    " \"no_rows\":[len(df)],\"is_consistent\":[do_check_consistency()],\"is_complete\":[1]\n",
    "} )\n",
    "addETLTrans(dfTran.to_records(index=False) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
