{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imported Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import psycopg2.extras as extras\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime,timezone\n",
    "import time\n",
    "from dateutil import tz\n",
    "\n",
    "import os\n",
    "import sys \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "from google.cloud.exceptions import NotFound\n",
    "from google.api_core.exceptions import BadRequest\n",
    "\n",
    "import os\n",
    "import sys \n",
    "import shutil\n",
    "\n",
    "import CheckDataCons_DB_BQ as check_data  \n",
    "\n",
    "import sqlite3\n",
    "\n",
    "from configupdater import ConfigUpdater\n",
    "# pip install ConfigUpdater\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "import LoadPGToBQ_BQStorageAPI as  bq_cdc_stream_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "is_py=False\n",
    "check_consistency=False\n",
    "time_wait_for_bq=30\n",
    "view_name = \"pmr_project\"\n",
    "log = \"models_logging_change\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View name to load to BQ :pmr_project\n"
     ]
    }
   ],
   "source": [
    "isFirstLoad=False\n",
    "                             \n",
    "\n",
    "if is_py:\n",
    "    press_Y=''\n",
    "    ok=False\n",
    "\n",
    "    if len(sys.argv) > 1:\n",
    "        view_name=sys.argv[1]\n",
    "    else:\n",
    "        print(\"Enter the following input: \")\n",
    "        view_name = input(\"View Table Name : \")\n",
    "print(f\"View name to load to BQ :{view_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imported date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTC: 2024-01-18 17:01:12 For This Import\n"
     ]
    }
   ],
   "source": [
    "dt_imported=datetime.now(timezone.utc) # utc\n",
    "dt_imported=datetime.strptime(dt_imported.strftime(\"%Y-%m-%d %H:%M:%S\"),\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"UTC: {dt_imported} For This Import\")\n",
    "\n",
    "str_dt_imported=dt_imported.strftime(\"%Y-%m-%d %H:%M:%S\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Configuration File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test config,env file and key to be used ,all of used key  are existing.\n",
    "def get_config_file():\n",
    "\n",
    "    try:\n",
    "        cfg_path=\"cfg_last_import\"\n",
    "        env_path='.env'\n",
    "        data_base_file=\"etl_web_admin/bq_cdc_etl_transaction.db\"\n",
    "\n",
    "        connection = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "\n",
    "        config = dotenv_values(dotenv_path=env_path)\n",
    "\n",
    "        updater = ConfigUpdater()\n",
    "        updater.read(os.path.join(cfg_path,f\"{view_name}.cfg\"))\n",
    "\n",
    "    except Exception as e:\n",
    "      raise e    \n",
    "    finally:\n",
    "\n",
    "        if connection:\n",
    "            connection.close()\n",
    "\n",
    "    return  config,updater,data_base_file\n",
    "\n",
    "config,updater,data_base_file=get_config_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Last Import to retrive data after that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTC:2024-01-18 16:07:59  Of Last Import\n"
     ]
    }
   ],
   "source": [
    "last_imported=datetime.strptime(updater[\"metadata\"][view_name].value,\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"UTC:{last_imported}  Of Last Import\")\n",
    "\n",
    "# local_zone = tz.tzlocal()\n",
    "# last_imported = last_imported.astimezone(local_zone)\n",
    "# print(f\"Local Asia/Bangkok:{last_imported}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Table Namd and StoreProc on BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project\n",
      "merge_project\n"
     ]
    }
   ],
   "source": [
    "# pmr for merging\n",
    "# xyz for bq-storage-api\n",
    "data_name=view_name.replace(\"pmr_\",\"\").replace(\"xyz_\",\"\")\n",
    "sp_name=f\"merge_{data_name}\"\n",
    "print(data_name)\n",
    "print(sp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sqlite3.register_adapter(np.int64, lambda val: int(val))\n",
    "sqlite3.register_adapter(np.int32, lambda val: int(val))\n",
    "\n",
    "\n",
    "def list_data_sqlite(sql):\n",
    "    try:\n",
    "        conn = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "        print(sql)\n",
    "        df_item=pd.read_sql_query(sql, conn)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to insert etl_transaction table\", str(e))\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "    return df_item\n",
    "\n",
    "def addETLTrans(recordList):\n",
    "    try:\n",
    "        sqliteConnection = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "        cursor = sqliteConnection.cursor()\n",
    "        sqlite_insert_query = \"\"\"\n",
    "        INSERT INTO etl_transaction\n",
    "        (trans_datetime, view_source_id,no_rows,is_consistent,is_complete)  \n",
    "        VALUES (?,?,?,?,?);\n",
    "         \"\"\"\n",
    "        cursor.executemany(sqlite_insert_query, recordList)\n",
    "        print(\"Done ETL Trasaction\")\n",
    "        sqliteConnection.commit()\n",
    "        cursor.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Failed to insert etl_transaction table\", str(e))\n",
    "    finally:\n",
    "        if sqliteConnection:\n",
    "            sqliteConnection.close()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postgres &BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_postgres_conn():\n",
    " try:\n",
    "  conn = psycopg2.connect(\n",
    "        database=config['DATABASES_NAME'], user=config['DATABASES_USER'],\n",
    "      password=config['DATABASES_PASSWORD'], host=config['DATABASES_HOST']\n",
    "     )\n",
    "  return conn\n",
    "\n",
    " except Exception as error:\n",
    "  print(error)      \n",
    "  raise error\n",
    "def list_data(sql,params,connection):\n",
    " df=None   \n",
    " with connection.cursor() as cursor:\n",
    "    \n",
    "    if params is None:\n",
    "       cursor.execute(sql)\n",
    "    else:\n",
    "       cursor.execute(sql,params)\n",
    "    \n",
    "    columns = [col[0] for col in cursor.description]\n",
    "    dataList = [dict(zip(columns, row)) for row in cursor.fetchall()]\n",
    "    df = pd.DataFrame(data=dataList) \n",
    " return df \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get View Source  to set configuration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from view_source where name='pmr_project' limit 1\n",
      "id                                                                           1\n",
      "name                                                               pmr_project\n",
      "main_source_table_name                                             app_project\n",
      "load_type                                                                merge\n",
      "app_conten_type_id                                                           7\n",
      "app_key_name                                                        project_id\n",
      "app_changed_field_mapping    enq_id, \\r\\nproject_name,   \\r\\nproject_start ...\n",
      "app_fk_name_list                                                    company_id\n",
      "app_datetime_field_list                                                       \n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def get_view_source(name):\n",
    "    try:\n",
    "        sql=f\"select * from view_source where name='{name}' limit 1\"\n",
    "        dfView=list_data_sqlite(sql)\n",
    "        if dfView.empty==False:\n",
    "           view_source=dfView.iloc[0,:]\n",
    "        else:\n",
    "            error=f\"Not found {view_name} view\"\n",
    "            raise Exception(error)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to insert etl_transaction table\", str(e))\n",
    "        raise Exception(e)\n",
    "        \n",
    "    return view_source\n",
    "view_source= get_view_source(view_name)\n",
    "print(view_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app_project #LoadyType:merge # ContentyTypeID:7 # KeyName:project_id # SP:merge_project\n",
      "['enq_id', 'project_name', 'project_start', 'project_end', 'company_id', 'has_pm']\n",
      "['company_id', 'project_id']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "admin_view_id=view_source['id']\n",
    "content_id=view_source['app_conten_type_id']\n",
    "view_name_id=view_source['app_key_name']\n",
    "\n",
    "app_table_name=view_source['main_source_table_name']\n",
    "\n",
    "changed_field_mapping=view_source['app_changed_field_mapping'].strip().split(\",\")\n",
    "changed_field_mapping = [ x.replace(\" \", \"\").replace(\"\\r\", \"\").replace(\"\\n\", \"\") for x  in changed_field_mapping] \n",
    "\n",
    "way=view_source['load_type'] # 1=\"merge\"  or \"bq-storage-api\"\n",
    "\n",
    "pk_fk_list=[]\n",
    "if view_source['app_fk_name_list'] is not None and view_source['app_fk_name_list']!='':\n",
    "    pk_fk_list=view_source['app_fk_name_list'].strip().split(\",\")\n",
    "    pk_fk_list= [ x.replace(\" \", \"\").replace(\"\\r\", \"\").replace(\"\\n\", \"\") for x  in  pk_fk_list] \n",
    "pk_fk_list.append(view_name_id)\n",
    "\n",
    "datetime_list=[]\n",
    "if view_source['app_datetime_field_list'] is not None and view_source['app_datetime_field_list'] !='':\n",
    "    datetime_list=view_source['app_datetime_field_list'].strip().split(\",\")\n",
    "    datetime_list= [ x.replace(\" \", \"\").replace(\"\\r\", \"\").replace(\"\\n\", \"\") for x  in  datetime_list] \n",
    "\n",
    "\n",
    "print(f\"{app_table_name} #LoadyType:{way} # ContentyTypeID:{content_id} # KeyName:{view_name_id} # SP:{sp_name}\")\n",
    "print(changed_field_mapping)\n",
    "print(pk_fk_list)\n",
    "print(datetime_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filed/Columns Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_fileds_in_view_source_in_web_admin_existing_in_database(x_name,x_list):\n",
    "    \n",
    "    sqlCheck=f\"SELECT column_name FROM information_schema.columns WHERE table_name = '{x_name}'\"\n",
    "    listColTable=list_data(sqlCheck,None,get_postgres_conn())\n",
    "    \n",
    "    listColTable=listColTable[\"column_name\"].tolist()\n",
    "    print(listColTable)\n",
    "    print(\"=====================================\")\n",
    "    print(x_list)\n",
    "    x_check = all(elem in listColTable for elem in x_list)\n",
    "    \n",
    "    return x_check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All PK and FK in ViewSource table on WebAdmin must be in view pmr_project\n",
      "['updated_at', 'company_id', 'has_pm', 'project_id', 'project_start', 'project_end', 'enq', 'project_name', 'company']\n",
      "=====================================\n",
      "['company_id', 'project_id']\n",
      "True\n",
      "All Columns Mapping to check changed data in ViewSource table on WebAdmin must be in table app_project\n",
      "['id', 'updated_at', 'company_id', 'project_end', 'project_start', 'has_pm', 'is_dummy', 'enq_id', 'project_name', 'customer_po', 'pm_des', 'contract_no']\n",
      "=====================================\n",
      "['enq_id', 'project_name', 'project_start', 'project_end', 'company_id', 'has_pm']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(f\"All PK and FK in ViewSource table on WebAdmin must be in view {view_name}\")\n",
    "pk_fk_check=check_fileds_in_view_source_in_web_admin_existing_in_database(view_name,pk_fk_list)\n",
    "if pk_fk_check:\n",
    "    print(pk_fk_check)\n",
    "else:\n",
    "    raise Exception(f\"There are some columns are not in {view_name}\")\n",
    "\n",
    "print(f\"All Columns Mapping to check changed data in ViewSource table on WebAdmin must be in table {app_table_name}\")\n",
    "chanagd_mapping_check=check_fileds_in_view_source_in_web_admin_existing_in_database(app_table_name,changed_field_mapping)\n",
    "if  chanagd_mapping_check:\n",
    "    print(chanagd_mapping_check)   \n",
    "else:\n",
    "    raise Exception(f\"There are some columns are not in {app_table_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigQuery Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table pongthorn.SMartData_Temp.temp_project already exists.\n",
      "Table pongthorn.SMartData_Temp.temp_project already exists.\n",
      "[('project_id', 'INTEGER'), ('enq', 'STRING'), ('project_name', 'STRING'), ('project_start', 'DATE'), ('project_end', 'DATE'), ('company_id', 'INTEGER'), ('company', 'STRING'), ('has_pm', 'BOOLEAN'), ('action', 'STRING')]\n",
      "[('project_id', 'INTEGER'), ('enq', 'STRING'), ('project_name', 'STRING'), ('project_start', 'DATE'), ('project_end', 'DATE'), ('company_id', 'INTEGER'), ('company', 'STRING'), ('has_pm', 'BOOLEAN'), ('is_deleted', 'BOOLEAN'), ('update_at', 'TIMESTAMP')]\n"
     ]
    }
   ],
   "source": [
    "projectId=config['PROJECT_ID']  # smart-data-ml  or kku-intern-dataai or ponthorn\n",
    "credential_file=config['PROJECT_CREDENTIAL_FILE']\n",
    "# C:\\Windows\\smart-data-ml-91b6f6204773.json\n",
    "# C:\\Windows\\kku-intern-dataai-a5449aee8483.json\n",
    "# C:\\Windows\\pongthorn-5decdc5124f5.json\n",
    "\n",
    "dataset_id=config['TEMP_DATASET'] # 'SMartData_Temp'  'PMReport_Temp'\n",
    "main_dataset_id=config['MAIN_DATASET']  # ='SMartDataAnalytics'  'PMReport_Main'\n",
    "\n",
    "table_name=f\"temp_{data_name}\" #can change in (\"name\") to temp table\n",
    "table_id = f\"{projectId}.{dataset_id}.{table_name}\"\n",
    "\n",
    "\n",
    "\n",
    "main_table_name=data_name\n",
    "main_table_id = f\"{projectId}.{main_dataset_id}.{main_table_name}\"\n",
    "\n",
    "# https://cloud.google.com/bigquery/docs/reference/rest/v2/Job\n",
    "to_bq_mode=\"WRITE_EMPTY\"\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(credential_file)\n",
    "client = bigquery.Client(credentials= credentials,project=projectId)\n",
    "\n",
    "def check_existing_table_return_schema(clien,x_table_id):\n",
    "    try:\n",
    "        table = client.get_table(x_table_id)  # Make an API request.\n",
    "        print(\"Table {} already exists.\".format(table_id))\n",
    "\n",
    "        schema = table.schema\n",
    "        listTableSchema = [(field.name, field.field_type) for field in schema]\n",
    "\n",
    "        return listTableSchema\n",
    "    \n",
    "    except NotFound as e:\n",
    "        print(\"Table {} does not exist.\".format(table_id))\n",
    "        raise e\n",
    "\n",
    "table_schema=  check_existing_table_return_schema(client,table_id)    \n",
    "main_table_schema=  check_existing_table_return_schema(client,main_table_id)   \n",
    "print(table_schema)\n",
    "print(main_table_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bq_table():\n",
    " try:\n",
    "    table=client.get_table(table_id)  # Make an API request.\n",
    "    print(\"Table {} already exists.\".format(table_id))\n",
    "    print(table.schema)\n",
    "    return True\n",
    " except NotFound:\n",
    "    raise Exception(\"Table {} is not found.\".format(table_id))\n",
    "    \n",
    "\n",
    "def insertDataFrameToBQ(df_trasns):\n",
    "    try:\n",
    "        job_config = bigquery.LoadJobConfig(write_disposition=to_bq_mode,)\n",
    "        job = client.load_table_from_dataframe(df_trasns, table_id, job_config=job_config)\n",
    "        \n",
    "        job.result()  # Wait for the job to complete.\n",
    "        print(\"Total \", len(df_trasns), f\"Imported data to {table_id} on bigquery successfully\")\n",
    "\n",
    "    except BadRequest as err:\n",
    "        # Handle the BadRequest exception\n",
    "        print(\"BadRequest error:\", err)\n",
    "        print(\"Error details:\", err.errors)  # Access detailed error information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Data Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_check_consistency():\n",
    "    check_result=True\n",
    "    if check_consistency:\n",
    "         print(\"Wait in a while for biqguery to update\")\n",
    "         time.sleep(time_wait_for_bq)\n",
    "         print(\"Check data consistency betwwen database and bigquery\")\n",
    "         result=check_data.check_data_consistency_db_bq(view_name)\n",
    "         if result:\n",
    "            print(\"if result=True , view csv file in check_db_bq  data_consistence_check\")  \n",
    "            print(\"send email to admin to investigate somthing wrong.\")\n",
    "            check_result=False\n",
    "         else:\n",
    "            print(f\"Data has been consistent between {config['DATABASES_NAME']} and {main_table_id}\")\n",
    "    else:\n",
    "        print(\"Disable checking data consistency feature.\")\n",
    "            \n",
    "    return int(check_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Add transaction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_tran(x_no_rows,x_is_complete):\n",
    "    print(\"Add transaction.\")\n",
    "    dfTran=pd.DataFrame(data={\n",
    "            \"trans_datetime\":[str_dt_imported],\"view_source_id\":[admin_view_id],\n",
    "            \"no_rows\":[x_no_rows],\"is_consistent\":[do_check_consistency()],\"is_complete\":[x_is_complete]\n",
    "            } )\n",
    "    addETLTrans(dfTran.to_records(index=False) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check whether it is the first loading?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def checkFirstLoad():\n",
    "    print(\"If the main table is empty , so the action of each row  must be 'added' on temp table\")\n",
    "    rows_iter   = client.list_rows(main_table_id, max_results=1) \n",
    "    no_main=len(list(rows_iter))\n",
    "    if no_main==0:\n",
    "     isFirstLoad=True\n",
    "     print(f\"This is the first loaing , so there is No DATA in {main_table_id}, we load all rows from {view_name} to import into {table_id} action will be 'added' \")\n",
    "    else:\n",
    "     isFirstLoad=False   \n",
    "    return isFirstLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the main table is empty , so the action of each row  must be 'added' on temp table\n",
      "IsFirstLoad=False for project\n"
     ]
    }
   ],
   "source": [
    "isFirstLoad=checkFirstLoad()\n",
    "print(f\"IsFirstLoad={isFirstLoad} for {data_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For The next Load\n",
    "* Get data from model log based on condition last_imported and table\n",
    "* Get all actions from log table by selecting unique object_id and setting by doing something as logic\n",
    "* Create  id and action dataframe form filtered rows from log table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_model_log(x_last_imported,x_content_id):\n",
    "    sql_log = f\"\"\"\n",
    "    SELECT object_id, action,TO_CHAR(date_created,'YYYY-MM-DD HH24:MI:SS') as date_created ,changed_data\n",
    "    FROM {log}\n",
    "    WHERE date_created  AT time zone 'utc' >= '{x_last_imported}' AND content_type_id = {x_content_id} \n",
    "    ORDER BY object_id, date_created\n",
    "    \"\"\"\n",
    "    print(sql_log)\n",
    "\n",
    "\n",
    "    # Asia/Bangkok \n",
    "    lf = list_data(sql_log, None, get_postgres_conn())\n",
    "    print(f\"Retrieve all rows after {last_imported}\")\n",
    "    print(lf.info())\n",
    "    return lf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Change in Mappping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def findChangeInListMapping(changed_data):\n",
    "    # print(type(changed_data))\n",
    "    # print(changed_data)\n",
    "    x=False\n",
    "    for key in changed_data.keys():\n",
    "        # print(key)\n",
    "        if key in changed_field_mapping :\n",
    "            print(f\"{key} in {changed_field_mapping}\")\n",
    "            x= True\n",
    "\n",
    "    return x\n",
    "    \n",
    "\n",
    "def check_no_changes_to_columns_view_only_changed_action(dfAction,x_view_name,_x_key_name):\n",
    "    \"\"\"\n",
    "    Check dataframe from log model that contain only changed action to select changed fields on view.\n",
    "    Gather id no any changes based on  changed_field_mapping to get rid of it from list to import to BQ\n",
    "    \"\"\"\n",
    "\n",
    "    listACtion=dfAction[\"action\"].unique().tolist()\n",
    "    if len(listACtion)==1 and listACtion[0]=='changed':\n",
    "        print(\"#######################Find Some Changes#############################\")\n",
    "        print(\"Process dataframe containing only all changed action\")\n",
    "        dfAction['x']=dfAction['changed_data'].apply(findChangeInListMapping)\n",
    "        print(dfAction[['object_id','x','changed_data']])\n",
    "        \n",
    "        any_rows_match = dfAction['x'] ==True\n",
    "        match_x=any_rows_match.any()\n",
    "        print(\"Check whether at least one row in a DataFrame matches a specific criteria\")\n",
    "        print(match_x)\n",
    "        # there is at least one change in mapping changed_field_mapping : return false\n",
    "        if match_x:\n",
    "            return False\n",
    "        # there is no any change in mapping changed_field_mapping : return true  \n",
    "        else: # return to caller for deleteing from list\n",
    "            return True\n",
    "        print(\"#####################################################################\")\n",
    "    else:\n",
    "        return False\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "listForRemove=[]\n",
    "def select_actual_action(lf):\n",
    "    listIDs=lf[\"object_id\"].unique().tolist()\n",
    "    listUpdateData=[]\n",
    "    for id in listIDs:\n",
    "        lfTemp=lf.query(\"object_id==@id\")\n",
    "        print(f\"--------------------{id}---------------------------------\")\n",
    "        print(lfTemp)\n",
    "        print(f\"--------------------end---------------------------------\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        x=check_no_changes_to_columns_view_only_changed_action(lfTemp,view_name,view_name_id)\n",
    "        if x==True:\n",
    "           print(f\"RemoveID {id}\") \n",
    "           listForRemove.append(id) \n",
    "\n",
    "\n",
    "        first_row = lfTemp.iloc[0]\n",
    "        last_row = lfTemp.iloc[-1]\n",
    "        # print(first_row)\n",
    "        # print(last_row)\n",
    "\n",
    "        if len(lfTemp)==1:\n",
    "            listUpdateData.append([id,first_row[\"action\"]])\n",
    "        else:\n",
    "            if first_row[\"action\"] == \"added\" and last_row[\"action\"] == \"deleted\":\n",
    "                continue\n",
    "            elif first_row[\"action\"] == \"added\" and last_row[\"action\"] != \"deleted\":\n",
    "                listUpdateData.append([id,\"added\"])\n",
    "            else : listUpdateData.append([id,last_row[\"action\"]])\n",
    "\n",
    "    print(\"Convert listUpdate to dataframe\")\n",
    "    dfUpdateData = pd.DataFrame(listUpdateData, columns= ['id', 'action'])\n",
    "    dfUpdateData['id'] = dfUpdateData['id'].astype('int64')\n",
    "    dfUpdateData=dfUpdateData.sort_values(by=\"id\")\n",
    "    dfUpdateData=dfUpdateData.reset_index(drop=True)\n",
    "\n",
    "    return dfUpdateData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process finding actual action, if there is no any rows in model logging then exit()\n",
      "\n",
      "    SELECT object_id, action,TO_CHAR(date_created,'YYYY-MM-DD HH24:MI:SS') as date_created ,changed_data\n",
      "    FROM models_logging_change\n",
      "    WHERE date_created  AT time zone 'utc' >= '2024-01-18 16:07:59' AND content_type_id = 7 \n",
      "    ORDER BY object_id, date_created\n",
      "    \n",
      "Retrieve all rows after 2024-01-18 16:07:59\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2 entries, 0 to 1\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   object_id     2 non-null      object\n",
      " 1   action        2 non-null      object\n",
      " 2   date_created  2 non-null      object\n",
      " 3   changed_data  2 non-null      object\n",
      "dtypes: object(4)\n",
      "memory usage: 192.0+ bytes\n",
      "None\n",
      "Get row imported from model log to set action\n",
      "--------------------787---------------------------------\n",
      "  object_id   action         date_created  \\\n",
      "0       787  changed  2024-01-18 23:30:21   \n",
      "1       787  changed  2024-01-18 23:30:45   \n",
      "\n",
      "                                        changed_data  \n",
      "0  {'updated_at': {'new': '2024-01-18T16:30:21.75...  \n",
      "1  {'has_pm': {'new': False, 'old': True}, 'pm_de...  \n",
      "--------------------end---------------------------------\n",
      "#######################Find Some Changes#############################\n",
      "Process dataframe containing only all changed action\n",
      "project_name in ['enq_id', 'project_name', 'project_start', 'project_end', 'company_id', 'has_pm']\n",
      "has_pm in ['enq_id', 'project_name', 'project_start', 'project_end', 'company_id', 'has_pm']\n",
      "  object_id     x                                       changed_data\n",
      "0       787  True  {'updated_at': {'new': '2024-01-18T16:30:21.75...\n",
      "1       787  True  {'has_pm': {'new': False, 'old': True}, 'pm_de...\n",
      "Check whether at least one row in a DataFrame matches a specific criteria\n",
      "True\n",
      "Convert listUpdate to dataframe\n",
      "Remove these Ids from dfModelLog : []\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1 entries, 0 to 0\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      1 non-null      int64 \n",
      " 1   action  1 non-null      object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 24.0+ bytes\n",
      "None\n",
      "    id   action\n",
      "0  787  changed\n",
      "[787]\n"
     ]
    }
   ],
   "source": [
    "print(\"Process finding actual action, if there is no any rows in model logging then exit()\")\n",
    "\n",
    "if isFirstLoad==False:\n",
    "    listModelLogObjectIDs=[]\n",
    "    dfModelLog=list_model_log(last_imported,content_id)\n",
    "    \n",
    "    if dfModelLog.empty==True:\n",
    "\n",
    "        add_tran(0,1)\n",
    "        print(\"No row to be imported prior to processing finding actual final aciton.\")\n",
    "        exit()\n",
    "    else:\n",
    "        print(\"Get row imported from model log to set action\") \n",
    "        dfModelLog=select_actual_action( dfModelLog)\n",
    "        listForRemove=[int(id) for id in listForRemove ]\n",
    "        print(f\"Remove these Ids from dfModelLog : {listForRemove}\")\n",
    "        dfModelLog=dfModelLog.query(\"id not in @listForRemove\")\n",
    "        listModelLogObjectIDs=dfModelLog['id'].tolist()\n",
    "\n",
    "        print(dfModelLog.info())\n",
    "        print(dfModelLog)       \n",
    "        print(listModelLogObjectIDs) \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load view by object id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrive_next_data_from_view(x_view,x_id,x_listModelLogObjectIDs):\n",
    "    obbjectID_str_list = ', '.join([\"'{}'\".format(value) for value in x_listModelLogObjectIDs])   \n",
    "    sql_view=f\"select *  from {x_view}  where {x_id} in ({obbjectID_str_list })\"\n",
    "\n",
    "    print(sql_view)\n",
    "    df=list_data(sql_view,None,get_postgres_conn())\n",
    "    # in case of all deleted item , it will return empty dataframe\n",
    "    if df.empty==True:\n",
    "        return df\n",
    "    df=df.drop(columns='updated_at')\n",
    "    return df \n",
    "    \n",
    "def retrive_first_data_from_view(x_view,x_last_imported):\n",
    "     sql_view=f\"select *  from {x_view}  where  updated_at AT time zone 'utc' >= '{x_last_imported}'\"\n",
    "     print(sql_view)\n",
    "     df=list_data(sql_view,None,get_postgres_conn())\n",
    "     if df.empty==True:\n",
    "            return df\n",
    "     df=df.drop(columns='updated_at')\n",
    "     df['action']='added'\n",
    "     return df   \n",
    "\n",
    "# it is used for pn;yall delted items \n",
    "def retrive_one_row_from_view_to_gen_df_schema_for_all_deleted_action(x_view):\n",
    "    sql_view=f\"select *  from {x_view}  limit 1\"\n",
    "    print(sql_view)\n",
    "    df=list_data(sql_view,None,get_postgres_conn())\n",
    "    df=df.drop(columns='updated_at')\n",
    "    return df\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before process finding actual action, if there is no any rows after removing id \n",
      "select *  from pmr_project  where project_id in ('787')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   project_id     1 non-null      int64 \n",
      " 1   enq            1 non-null      object\n",
      " 2   project_name   1 non-null      object\n",
      " 3   project_start  1 non-null      object\n",
      " 4   project_end    1 non-null      object\n",
      " 5   company_id     1 non-null      int64 \n",
      " 6   company        1 non-null      object\n",
      " 7   has_pm         1 non-null      bool  \n",
      "dtypes: bool(1), int64(2), object(5)\n",
      "memory usage: 185.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Before process finding actual action, if there is no any rows after removing id \")\n",
    "\n",
    "if isFirstLoad:\n",
    "    df=retrive_first_data_from_view(view_name,last_imported)\n",
    "    if df.empty==True:\n",
    "    # create dataframe and addETLTrans 0 row \n",
    "        print(\"No row to be imported.\")\n",
    "        exit()\n",
    "\n",
    "# after process actual aciton, if there is some rows that have some changes excluding in changed data mapping \n",
    "else:  \n",
    "    if len(listModelLogObjectIDs)>0 :  \n",
    "     df=retrive_next_data_from_view(view_name,view_name_id,listModelLogObjectIDs)  \n",
    "     if df.empty==True:\n",
    "        print(\"All deleted items, we will Get schema from {} to create empty dataframe with schema.\")\n",
    "        df=retrive_one_row_from_view_to_gen_df_schema_for_all_deleted_action(view_name)\n",
    "        # this id has been included in listModelLogObjectIDs which contain deleted action , so we can use it as schema generation\n",
    "    else:\n",
    "        add_tran(0,1)\n",
    "        print(\"No row to be imported after processing finding actual final aciton.\")\n",
    "        exit()    \n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transaformation\n",
    "* IF The first load then add actio='Added'\n",
    "* IF The nextload then Merge LogDF and ViewDF and add deleted row \n",
    "  * Get Deleted Items  to Create deleted dataframe by using listDeleted\n",
    "  * If there is one deletd row then  we will merge it to master dataframe\n",
    "* IF the next load has only deleted action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_acutal_action_to_df_at_next(df,dfUpdateData,x_view,x_id):\n",
    "    # merget model log(id and action) to data view\n",
    "    # if  dfUpdateData  contain only deleted action\n",
    "    # we will merge to get datafdame shcema, it can perform inner without have actual data fram view\n",
    "    merged_df = pd.merge(df, dfUpdateData, left_on=view_name_id, right_on='id', how='inner')\n",
    "    merged_df = merged_df.drop(columns=['id'])\n",
    "\n",
    "    listAllAction=dfUpdateData['id'].tolist()\n",
    "    print(f\"List {listAllAction} all action\")\n",
    "    \n",
    "    listSeleted = merged_df[view_name_id].tolist()\n",
    "    print(f\"List  {x_view}   {listSeleted} from {x_view} exluding deleted action\")\n",
    "    \n",
    "    allActionSet = set(listAllAction)\n",
    "    anotherSet = set(listSeleted)\n",
    "    \n",
    "    listDeleted = list(allActionSet.symmetric_difference(anotherSet))\n",
    "    print(f\"List deleted {listDeleted}\")\n",
    "    \n",
    "    # Test List  select by view + List deeleted = List All Action\n",
    "\n",
    "    if len(listDeleted)>0:\n",
    "        print(\"There are some deleted rows\")\n",
    "        dfDeleted=pd.DataFrame(data=listDeleted,columns=[view_name_id])\n",
    "        dfDeleted['action']='deleted'\n",
    "        print(dfDeleted)\n",
    "        merged_df=pd.concat([merged_df,dfDeleted],axis=0)\n",
    "\n",
    "    else:\n",
    "        print(\"No row deleted\")\n",
    "\n",
    "    return merged_df    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List [787] all action\n",
      "List  pmr_project   [787] from pmr_project exluding deleted action\n",
      "List deleted []\n",
      "No row deleted\n",
      "   project_id                    enq project_name project_start project_end  \\\n",
      "0         787  ENQ/65-Yip-DemoTest#5      XYZ&ABC    2024-01-01  2024-02-01   \n",
      "\n",
      "   company_id company  has_pm   action  \n",
      "0           1     YIP   False  changed  \n"
     ]
    }
   ],
   "source": [
    "if isFirstLoad==False:\n",
    " df=add_acutal_action_to_df_at_next(df,dfModelLog,view_name,view_name_id)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check duplicate ID & reset index & convert all pk&fk to int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Step :Check duplicate ID & reset index & convert all pk&fk to int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Last Step :Check duplicate ID & reset index & convert all pk&fk to int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no duplicate project_id ID\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   project_id     1 non-null      Int64 \n",
      " 1   enq            1 non-null      object\n",
      " 2   project_name   1 non-null      object\n",
      " 3   project_start  1 non-null      object\n",
      " 4   project_end    1 non-null      object\n",
      " 5   company_id     1 non-null      Int64 \n",
      " 6   company        1 non-null      object\n",
      " 7   has_pm         1 non-null      bool  \n",
      " 8   action         1 non-null      object\n",
      "dtypes: Int64(2), bool(1), object(6)\n",
      "memory usage: 195.0+ bytes\n",
      "None\n",
      "   project_id                    enq project_name project_start project_end  \\\n",
      "0         787  ENQ/65-Yip-DemoTest#5      XYZ&ABC    2024-01-01  2024-02-01   \n",
      "\n",
      "   company_id company  has_pm   action  \n",
      "0           1     YIP   False  changed  \n"
     ]
    }
   ],
   "source": [
    "hasDplicateIDs = df[view_name_id].duplicated().any()\n",
    "if  hasDplicateIDs:\n",
    " raise Exception(\"There are some duplicate id on dfUpdateData\")\n",
    "else:\n",
    " print(f\"There is no duplicate {view_name_id} ID\")  \n",
    "\n",
    "if len(pk_fk_list)>0:\n",
    " df[pk_fk_list] = df[pk_fk_list].astype('Int64')\n",
    "\n",
    "# merged_df['imported_at']=dt_imported\n",
    "df=df.reset_index(drop=True  )\n",
    "print(df.info())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schema Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column name validation\n",
      "Verify column name for temp(Merge-Sol)\n",
      "temp table: ['project_id', 'enq', 'project_name', 'project_start', 'project_end', 'company_id', 'company', 'has_pm', 'action']  == ['project_id', 'enq', 'project_name', 'project_start', 'project_end', 'company_id', 'company', 'has_pm', 'action']\n",
      "Verify column name for main(Both)\n",
      "temp table: ['project_id', 'enq', 'project_name', 'project_start', 'project_end', 'company_id', 'company', 'has_pm']  == ['project_id', 'enq', 'project_name', 'project_start', 'project_end', 'company_id', 'company', 'has_pm']\n"
     ]
    }
   ],
   "source": [
    "print(\"Column name validation\")\n",
    "def df_vs_bq(dFColsV,bQColsV):\n",
    "    if set(dFColsV) != set(bQColsV):\n",
    "     raise Exception(f\"temp table: {dFColsV} != {bQColsV}\")\n",
    "    else:\n",
    "     print(f\"temp table: {dFColsV}  == {bQColsV}\") \n",
    "    return True\n",
    "\n",
    "print(\"Verify column name for temp(Merge-Sol)\")\n",
    "if way==\"merge\":\n",
    "    tempDFColsV=df.columns.tolist()\n",
    "    tempBQColsV=[ col[0] for col in table_schema ]  \n",
    "    temp_result=df_vs_bq(tempDFColsV,tempBQColsV)\n",
    "\n",
    "print(\"Verify column name for main(Both)\") \n",
    "mainDFColsV=[ x for x in df.columns.tolist() if  x!=\"action\" ]\n",
    "mainBQColsV=[ col[0] for col in main_table_schema ] \n",
    "if way==\"merge\":\n",
    " mainBQColsV=[ x for x in mainBQColsV if x not in['is_deleted','update_at'] ] \n",
    "else:\n",
    " mainBQColsV=[ x for x in mainBQColsV if x not in['update_at',] ]    \n",
    "main_result=df_vs_bq(mainDFColsV,mainBQColsV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promp: how get table schema both field name and field type on Bigquery using python to store these values in nested list contain tuple?\n",
    "* https://docs.google.com/spreadsheets/d/1WrBvFsJpcm6UQ95pRJxGxk74VlDQHV1z0vgZQLeVotU/edit#gid=104041129\n",
    "* https://github.com/technqvi/MIS-FinData/blob/main/LoadDataFromOracleToBQ_Dev.ipynb\n",
    "\n",
    "## Error in code as detail\n",
    "* error if some column in dataframe contain null , it is interpreted to object type   depsite having excact tppy define in bigquery data schema such as actual date,docuemnt dat in pm_item or close_incident_date in incident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BQ_TO_DF_DATA_TYPE_MAPPING= \\\n",
    "# {\n",
    "#   \"STRING\":['object','str'],\n",
    "#   \"INTEGER\":['int','int32','int64'] ,\n",
    "#   \"FLOAT\":['float','float64'],\n",
    "#   \"BOOLEAN\":['bool'],\n",
    "#   \"TIMESTAMP\":['datetime64[ns]'],  \n",
    "#   \"DATETIME\":['datetime64[ns]'],   \n",
    "#   \"DATE\":['datetime64[ns]'],\n",
    "#   \"TIME\":['object','str'],\n",
    "    \n",
    "# }\n",
    "# dfTempBQSchema=pd.DataFrame(table_schema, columns=['name','type'])\n",
    "\n",
    "\n",
    "# for col_name, type_name in df[tempDFColsV].dtypes.items():\n",
    "#     found=False\n",
    "    \n",
    "#     df_type=str(type_name).lower()\n",
    "#     dfASDF= dfTempBQSchema.query(\"name==@col_name\")\n",
    "#     print(col_name,\"#Dataframe#\" ,df_type ) \n",
    "\n",
    "#     if  len(dfASDF)>0:\n",
    "#         bq_col_name=dfASDF.iloc[0,0]\n",
    "#         bq_col_type=dfASDF.iloc[0,1]\n",
    "#         print(bq_col_name,\"#BQ#\" ,bq_col_type ) \n",
    "#         if bq_col_type in BQ_TO_DF_DATA_TYPE_MAPPING and \\\n",
    "#             df_type in BQ_TO_DF_DATA_TYPE_MAPPING[bq_col_type]  : \n",
    "#              found=True\n",
    "#     if found==False:\n",
    "#         raise Exception(col_name,\"-#\" ,df_type,\" in dataframe didn't match in BQ\" )\n",
    "#     else:\n",
    "#         print(\"Found\")\n",
    "#     print(\"============================================================\")    \n",
    "     \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert data to BQ data frame & # Run StoreProcedure To Merge Temp&Main and Truncate Transaction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1#Ingest data into Bigquery using Merging-Sol\n",
      "Table pongthorn.SMartData_Temp.temp_project already exists.\n",
      "[SchemaField('project_id', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('enq', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('project_name', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('project_start', 'DATE', 'NULLABLE', None, None, (), None), SchemaField('project_end', 'DATE', 'NULLABLE', None, None, (), None), SchemaField('company_id', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('company', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('has_pm', 'BOOLEAN', 'NULLABLE', None, None, (), None), SchemaField('action', 'STRING', 'NULLABLE', None, None, (), None)]\n",
      "Total  1 Imported data to pongthorn.SMartData_Temp.temp_project on bigquery successfully\n",
      "2#Run StoreProcedure To Merge Temp&Main and Truncate Transaction.\n",
      " CALL `pongthorn.SMartDataAnalytics.merge_project`() \n"
     ]
    }
   ],
   "source": [
    "if way=='merge':\n",
    "    print(\"1#Ingest data into Bigquery using Merging-Sol\")\n",
    "    if get_bq_table():\n",
    "        try:\n",
    "            insertDataFrameToBQ(df)\n",
    "        except Exception as ex:\n",
    "            raise ex\n",
    "            \n",
    "    print(\"2#Run StoreProcedure To Merge Temp&Main and Truncate Transaction.\")\n",
    "    # https://cloud.google.com/bigquery/docs/transactions\n",
    "    sp_id_to_invoke=f\"\"\" CALL `{projectId}.{main_dataset_id}.{sp_name}`() \"\"\"\n",
    "    print(sp_id_to_invoke)    \n",
    "    sp_job = client.query(sp_id_to_invoke)\n",
    "\n",
    "else:\n",
    "    print(\"2#Ingest data into Bigquery using BQ-Storage-API-Sol\")\n",
    "    csv_file=f\"{data_name}_{way}.csv\"\n",
    "    df.to_csv(f\"{csv_file}\",index=False)\n",
    "    \n",
    "    datetime_check=True\n",
    "    if len(datetime_list)>0:\n",
    "        print(f\"All DateTime Columns to in ViewSource table on WebAdmin must be in table {view_name}\")\n",
    "        print(f\"There will be converted to  microseconds timestamp.\")\n",
    "        datetime_check=check_fileds_in_view_source_in_web_admin_existing_in_database(view_name,datetime_list)\n",
    "        if datetime_check:\n",
    "            print(datetime_check)\n",
    "        else:\n",
    "            raise Exception(f\"There are some columns are not in {app_table_name}\")\n",
    "    \n",
    "    resutl=bq_cdc_stream_loader.db_to_bq_by_bq_storage_api(\n",
    "    csv_file=csv_file,\n",
    "    view_name=view_name,view_name_id=view_name_id,\n",
    "    datetimeCols=datetime_list,pk_fkCols=pk_fk_list,\n",
    "    projectId=projectId,main_dataset_id= main_dataset_id,table_name=data_name,\n",
    "    dt_imported=dt_imported    \n",
    "    )   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# Update New Recenet Update to file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update New Recenet Update to file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ConfigUpdater [\n",
       "    <Section: 'metadata' [\n",
       "        <Option: pmr_project = '2024-01-18 17:01:12'>\n",
       "    ]>\n",
       "]>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Update New Recenet Update to file\")\n",
    "updater[\"metadata\"][view_name].value=dt_imported.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "updater.update_file() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-18 17:03:33.025289+00:00\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now(timezone.utc) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add ETL transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add ETLTrans n-row as dataframe\n",
      "Add transaction.\n",
      "Disable checking data consistency feature.\n",
      "Done ETL Trasaction\n"
     ]
    }
   ],
   "source": [
    "print(\"Add ETLTrans n-row as dataframe\")   \n",
    "add_tran(len(df),1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
